{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "# tfds.disable_progress_bar()xc\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # !tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100 101 102\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    # 0 PAD 1 UNK 100 CLS 101 SEP 102 MASK\n",
    "    vocab[103:-1] = vocab[100:-1]\n",
    "    vocab[100] = \"[cls]\"\n",
    "    vocab[101] = \"[sep]\"\n",
    "    vocab[102] = \"[mask]\"\n",
    "    vocab = vocab[2 : vocab_size]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "cls_token_id = vectorize_layer([\"[CLS]\"]).numpy()[0][0]\n",
    "sep_token_id = vectorize_layer([\"[SEP]\"]).numpy()[0][0]\n",
    "mask_token_id = vectorize_layer([\"[MASK]\"]).numpy()[0][0]\n",
    "print(cls_token_id, sep_token_id, mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "def get_masked_input(encoded_texts):\n",
    "    # get input_mask and input_type_ids\n",
    "    input_mask = np.zeros_like(encoded_texts)\n",
    "    input_mask[encoded_texts > 0] = 1\n",
    "    input_type_ids = np.zeros_like(encoded_texts)\n",
    "\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # Prepare input\n",
    "    input_word_ids = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    input_word_ids[inp_mask_2mask] = mask_token_id  \n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    input_word_ids[inp_mask_2random] = np.random.randint(\n",
    "        3, config.VOCAB_SIZE, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # get masked_lm_positions, masked_lm_ids, masked_lm_weights\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_ids = []\n",
    "    for (posistion, id) in zip(inp_mask, encoded_texts * inp_mask):\n",
    "        masked_lm_positions.append(np.where(posistion == True)[0])\n",
    "        masked_lm_ids.append(id[np.where(posistion == True)])\n",
    "\n",
    "    masked_lm_weights = [[1.0] * len(lm_id) for lm_id in masked_lm_ids]\n",
    "\n",
    "    return input_word_ids, input_mask, input_type_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_review = encode(all_data.review.values[:5])\n",
    "input_word_ids, input_mask, input_type_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights = get_masked_input(\n",
    "    x_all_review\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = tf.constant(input_word_ids)\n",
    "input_mask = tf.constant(input_mask)\n",
    "input_type_ids = tf.constant(input_type_ids)\n",
    "masked_lm_positions = tf.ragged.constant(masked_lm_positions).to_tensor()\n",
    "masked_lm_ids = tf.ragged.constant(masked_lm_ids).to_tensor()\n",
    "masked_lm_weights = tf.ragged.constant(masked_lm_weights).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m           \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    479\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0;32m--> 480\u001b[0;31m                   (element, type(element).__name__))\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for [[7, 11, 15, 29, 30, 33, 36, 40, 42, 45, 47, 62, 67, 70, 75, 77, 81, 93, 117, 132, 138, 139, 144, 145, 162, 175], [3, 12, 17, 27, 31, 43, 45, 57, 64, 72, 76, 77, 126], [1, 3, 7, 12, 15, 17, 27, 28, 30, 31, 32, 38, 54, 57, 59, 67, 70, 73, 81, 94, 96, 104], [0, 18, 24, 33, 34, 36, 38, 42, 44, 47, 50, 59, 69, 82, 84, 105, 107, 108, 123, 127, 130, 156, 157, 173, 176, 183, 201, 209, 211, 220, 230, 231, 247, 252], [10, 17, 24, 27, 28, 29, 42, 47, 59, 81, 87, 107, 113, 114, 119, 121, 122, 134, 137, 138, 143, 145]] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-1ba915771e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mlm_ds = tf.data.Dataset.from_tensor_slices(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0minput_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0mmlm_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlm_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3153\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3156\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (input_word_ids, input_mask, input_type_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {'attention_probs_dropout_prob': 0.01, \n",
    "               'hidden_act': 'gelu', \n",
    "               'hidden_dropout_prob': 0.1, \n",
    "               'hidden_size': 768, \n",
    "               'initializer_range': 0.02, \n",
    "               'intermediate_size': 3072, \n",
    "               'max_position_embeddings': 256, \n",
    "               'num_attention_heads': 12, \n",
    "               'num_hidden_layers': 12, \n",
    "               'type_vocab_size': 1, \n",
    "               'vocab_size': 30000}\n",
    "\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model, transformer_encoder, pretrainer_model = bert.bert_models.pretrain_model(\n",
    "    bert_config, seq_length=256, max_predictions_per_seq=34, use_next_sentence_label=False, return_core_pretrainer_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up epochs and steps\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "eval_batch_size = 128\n",
    "\n",
    "train_data_size = len(x_all_review)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_ds= (input_word_ids, input_mask, input_type_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_ds = {\n",
    "    \"input_word_ids\": input_word_ids, \n",
    "    \"input_mask\": input_mask, \n",
    "    \"input_type_ids\": input_type_ids, \n",
    "    \"masked_lm_positions\": masked_lm_positions, \n",
    "    \"masked_lm_ids\": masked_lm_ids, \n",
    "    \"masked_lm_weights\": masked_lm_weights, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /usr/local/lib/python3.6/dist-packages/official/nlp/optimization.py:181 apply_gradients\n        experimental_aggregate_gradients=experimental_aggregate_gradients)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:598 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py:79 filter_empty_gradients\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['word_embeddings/embeddings:0', 'position_embedding/embeddings:0', 'type_embeddings/embeddings:0', 'embeddings/layer_norm/gamma:0', 'embeddings/layer_norm/beta:0', 'transformer/layer_0/self_attention/query/kernel:0', 'transformer/layer_0/self_attention/query/bias:0', 'transformer/layer_0/self_attention/key/kernel:0', 'transformer/layer_0/self_attention/key/bias:0', 'transformer/layer_0/self_attention/value/kernel:0', 'transformer/layer_0/self_attention/value/bias:0', 'transformer/layer_0/self_attention/attention_output/kernel:0', 'transformer/layer_0/self_attention/attention_output/bias:0', 'transformer/layer_0/self_attention_layer_norm/gamma:0', 'transformer/layer_0/self_attention_layer_norm/beta:0', 'transformer/layer_0/intermediate/kernel:0', 'transformer/layer_0/intermediate/bias:0', 'transformer/layer_0/output/kernel:0', 'transformer/layer_0/output/bias:0', 'transformer/layer_0/output_layer_norm/gamma:0', 'transformer/layer_0/output_layer_norm/beta:0', 'transformer/layer_1/self_attention/query/kernel:0', 'transformer/layer_1/self_attention/query/bias:0', 'transformer/layer_1/self_attention/key/kernel:0', 'transformer/layer_1/self_attention/key/bias:0', 'transformer/layer_1/self_attention/value/kernel:0', 'transformer/layer_1/self_attention/value/bias:0', 'transformer/layer_1/self_attention/attention_output/kernel:0', 'transformer/layer_1/self_attention/attention_output/bias:0', 'transformer/layer_1/self_attention_layer_norm/gamma:0', 'transformer/layer_1/self_attention_layer_norm/beta:0', 'transformer/layer_1/intermediate/kernel:0', 'transformer/layer_1/intermediate/bias:0', 'transformer/layer_1/output/kernel:0', 'transformer/layer_1/output/bias:0', 'transformer/layer_1/output_layer_norm/gamma:0', 'transformer/layer_1/output_layer_norm/beta:0', 'transformer/layer_2/self_attention/query/kernel:0', 'transformer/layer_2/self_attention/query/bias:0', 'transformer/layer_2/self_attention/key/kernel:0', 'transformer/layer_2/self_attention/key/bias:0', 'transformer/layer_2/self_attention/value/kernel:0', 'transformer/layer_2/self_attention/value/bias:0', 'transformer/layer_2/self_attention/attention_output/kernel:0', 'transformer/layer_2/self_attention/attention_output/bias:0', 'transformer/layer_2/self_attention_layer_norm/gamma:0', 'transformer/layer_2/self_attention_layer_norm/beta:0', 'transformer/layer_2/intermediate/kernel:0', 'transformer/layer_2/intermediate/bias:0', 'transformer/layer_2/output/kernel:0', 'transformer/layer_2/output/bias:0', 'transformer/layer_2/output_layer_norm/gamma:0', 'transformer/layer_2/output_layer_norm/beta:0', 'transformer/layer_3/self_attention/query/kernel:0', 'transformer/layer_3/self_attention/query/bias:0', 'transformer/layer_3/self_attention/key/kernel:0', 'transformer/layer_3/self_attention/key/bias:0', 'transformer/layer_3/self_attention/value/kernel:0', 'transformer/layer_3/self_attention/value/bias:0', 'transformer/layer_3/self_attention/attention_output/kernel:0', 'transformer/layer_3/self_attention/attention_output/bias:0', 'transformer/layer_3/self_attention_layer_norm/gamma:0', 'transformer/layer_3/self_attention_layer_norm/beta:0', 'transformer/layer_3/intermediate/kernel:0', 'transformer/layer_3/intermediate/bias:0', 'transformer/layer_3/output/kernel:0', 'transformer/layer_3/output/bias:0', 'transformer/layer_3/output_layer_norm/gamma:0', 'transformer/layer_3/output_layer_norm/beta:0', 'transformer/layer_4/self_attention/query/kernel:0', 'transformer/layer_4/self_attention/query/bias:0', 'transformer/layer_4/self_attention/key/kernel:0', 'transformer/layer_4/self_attention/key/bias:0', 'transformer/layer_4/self_attention/value/kernel:0', 'transformer/layer_4/self_attention/value/bias:0', 'transformer/layer_4/self_attention/attention_output/kernel:0', 'transformer/layer_4/self_attention/attention_output/bias:0', 'transformer/layer_4/self_attention_layer_norm/gamma:0', 'transformer/layer_4/self_attention_layer_norm/beta:0', 'transformer/layer_4/intermediate/kernel:0', 'transformer/layer_4/intermediate/bias:0', 'transformer/layer_4/output/kernel:0', 'transformer/layer_4/output/bias:0', 'transformer/layer_4/output_layer_norm/gamma:0', 'transformer/layer_4/output_layer_norm/beta:0', 'transformer/layer_5/self_attention/query/kernel:0', 'transformer/layer_5/self_attention/query/bias:0', 'transformer/layer_5/self_attention/key/kernel:0', 'transformer/layer_5/self_attention/key/bias:0', 'transformer/layer_5/self_attention/value/kernel:0', 'transformer/layer_5/self_attention/value/bias:0', 'transformer/layer_5/self_attention/attention_output/kernel:0', 'transformer/layer_5/self_attention/attention_output/bias:0', 'transformer/layer_5/self_attention_layer_norm/gamma:0', 'transformer/layer_5/self_attention_layer_norm/beta:0', 'transformer/layer_5/intermediate/kernel:0', 'transformer/layer_5/intermediate/bias:0', 'transformer/layer_5/output/kernel:0', 'transformer/layer_5/output/bias:0', 'transformer/layer_5/output_layer_norm/gamma:0', 'transformer/layer_5/output_layer_norm/beta:0', 'transformer/layer_6/self_attention/query/kernel:0', 'transformer/layer_6/self_attention/query/bias:0', 'transformer/layer_6/self_attention/key/kernel:0', 'transformer/layer_6/self_attention/key/bias:0', 'transformer/layer_6/self_attention/value/kernel:0', 'transformer/layer_6/self_attention/value/bias:0', 'transformer/layer_6/self_attention/attention_output/kernel:0', 'transformer/layer_6/self_attention/attention_output/bias:0', 'transformer/layer_6/self_attention_layer_norm/gamma:0', 'transformer/layer_6/self_attention_layer_norm/beta:0', 'transformer/layer_6/intermediate/kernel:0', 'transformer/layer_6/intermediate/bias:0', 'transformer/layer_6/output/kernel:0', 'transformer/layer_6/output/bias:0', 'transformer/layer_6/output_layer_norm/gamma:0', 'transformer/layer_6/output_layer_norm/beta:0', 'transformer/layer_7/self_attention/query/kernel:0', 'transformer/layer_7/self_attention/query/bias:0', 'transformer/layer_7/self_attention/key/kernel:0', 'transformer/layer_7/self_attention/key/bias:0', 'transformer/layer_7/self_attention/value/kernel:0', 'transformer/layer_7/self_attention/value/bias:0', 'transformer/layer_7/self_attention/attention_output/kernel:0', 'transformer/layer_7/self_attention/attention_output/bias:0', 'transformer/layer_7/self_attention_layer_norm/gamma:0', 'transformer/layer_7/self_attention_layer_norm/beta:0', 'transformer/layer_7/intermediate/kernel:0', 'transformer/layer_7/intermediate/bias:0', 'transformer/layer_7/output/kernel:0', 'transformer/layer_7/output/bias:0', 'transformer/layer_7/output_layer_norm/gamma:0', 'transformer/layer_7/output_layer_norm/beta:0', 'transformer/layer_8/self_attention/query/kernel:0', 'transformer/layer_8/self_attention/query/bias:0', 'transformer/layer_8/self_attention/key/kernel:0', 'transformer/layer_8/self_attention/key/bias:0', 'transformer/layer_8/self_attention/value/kernel:0', 'transformer/layer_8/self_attention/value/bias:0', 'transformer/layer_8/self_attention/attention_output/kernel:0', 'transformer/layer_8/self_attention/attention_output/bias:0', 'transformer/layer_8/self_attention_layer_norm/gamma:0', 'transformer/layer_8/self_attention_layer_norm/beta:0', 'transformer/layer_8/intermediate/kernel:0', 'transformer/layer_8/intermediate/bias:0', 'transformer/layer_8/output/kernel:0', 'transformer/layer_8/output/bias:0', 'transformer/layer_8/output_layer_norm/gamma:0', 'transformer/layer_8/output_layer_norm/beta:0', 'transformer/layer_9/self_attention/query/kernel:0', 'transformer/layer_9/self_attention/query/bias:0', 'transformer/layer_9/self_attention/key/kernel:0', 'transformer/layer_9/self_attention/key/bias:0', 'transformer/layer_9/self_attention/value/kernel:0', 'transformer/layer_9/self_attention/value/bias:0', 'transformer/layer_9/self_attention/attention_output/kernel:0', 'transformer/layer_9/self_attention/attention_output/bias:0', 'transformer/layer_9/self_attention_layer_norm/gamma:0', 'transformer/layer_9/self_attention_layer_norm/beta:0', 'transformer/layer_9/intermediate/kernel:0', 'transformer/layer_9/intermediate/bias:0', 'transformer/layer_9/output/kernel:0', 'transformer/layer_9/output/bias:0', 'transformer/layer_9/output_layer_norm/gamma:0', 'transformer/layer_9/output_layer_norm/beta:0', 'transformer/layer_10/self_attention/query/kernel:0', 'transformer/layer_10/self_attention/query/bias:0', 'transformer/layer_10/self_attention/key/kernel:0', 'transformer/layer_10/self_attention/key/bias:0', 'transformer/layer_10/self_attention/value/kernel:0', 'transformer/layer_10/self_attention/value/bias:0', 'transformer/layer_10/self_attention/attention_output/kernel:0', 'transformer/layer_10/self_attention/attention_output/bias:0', 'transformer/layer_10/self_attention_layer_norm/gamma:0', 'transformer/layer_10/self_attention_layer_norm/beta:0', 'transformer/layer_10/intermediate/kernel:0', 'transformer/layer_10/intermediate/bias:0', 'transformer/layer_10/output/kernel:0', 'transformer/layer_10/output/bias:0', 'transformer/layer_10/output_layer_norm/gamma:0', 'transformer/layer_10/output_layer_norm/beta:0', 'transformer/layer_11/self_attention/query/kernel:0', 'transformer/layer_11/self_attention/query/bias:0', 'transformer/layer_11/self_attention/key/kernel:0', 'transformer/layer_11/self_attention/key/bias:0', 'transformer/layer_11/self_attention/value/kernel:0', 'transformer/layer_11/self_attention/value/bias:0', 'transformer/layer_11/self_attention/attention_output/kernel:0', 'transformer/layer_11/self_attention/attention_output/bias:0', 'transformer/layer_11/self_attention_layer_norm/gamma:0', 'transformer/layer_11/self_attention_layer_norm/beta:0', 'transformer/layer_11/intermediate/kernel:0', 'transformer/layer_11/intermediate/bias:0', 'transformer/layer_11/output/kernel:0', 'transformer/layer_11/output/bias:0', 'transformer/layer_11/output_layer_norm/gamma:0', 'transformer/layer_11/output_layer_norm/beta:0', 'pooler_transform/kernel:0', 'pooler_transform/bias:0', 'predictions/transform/logits/kernel:0', 'predictions/transform/logits/bias:0', 'cls/predictions/output_bias/bias:0', 'cls/predictions/transform/dense/kernel:0', 'cls/predictions/transform/dense/bias:0', 'cls/predictions/transform/LayerNorm/gamma:0', 'cls/predictions/transform/LayerNorm/beta:0'].\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-d2bd8abfb3c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /usr/local/lib/python3.6/dist-packages/official/nlp/optimization.py:181 apply_gradients\n        experimental_aggregate_gradients=experimental_aggregate_gradients)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:598 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py:79 filter_empty_gradients\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['word_embeddings/embeddings:0', 'position_embedding/embeddings:0', 'type_embeddings/embeddings:0', 'embeddings/layer_norm/gamma:0', 'embeddings/layer_norm/beta:0', 'transformer/layer_0/self_attention/query/kernel:0', 'transformer/layer_0/self_attention/query/bias:0', 'transformer/layer_0/self_attention/key/kernel:0', 'transformer/layer_0/self_attention/key/bias:0', 'transformer/layer_0/self_attention/value/kernel:0', 'transformer/layer_0/self_attention/value/bias:0', 'transformer/layer_0/self_attention/attention_output/kernel:0', 'transformer/layer_0/self_attention/attention_output/bias:0', 'transformer/layer_0/self_attention_layer_norm/gamma:0', 'transformer/layer_0/self_attention_layer_norm/beta:0', 'transformer/layer_0/intermediate/kernel:0', 'transformer/layer_0/intermediate/bias:0', 'transformer/layer_0/output/kernel:0', 'transformer/layer_0/output/bias:0', 'transformer/layer_0/output_layer_norm/gamma:0', 'transformer/layer_0/output_layer_norm/beta:0', 'transformer/layer_1/self_attention/query/kernel:0', 'transformer/layer_1/self_attention/query/bias:0', 'transformer/layer_1/self_attention/key/kernel:0', 'transformer/layer_1/self_attention/key/bias:0', 'transformer/layer_1/self_attention/value/kernel:0', 'transformer/layer_1/self_attention/value/bias:0', 'transformer/layer_1/self_attention/attention_output/kernel:0', 'transformer/layer_1/self_attention/attention_output/bias:0', 'transformer/layer_1/self_attention_layer_norm/gamma:0', 'transformer/layer_1/self_attention_layer_norm/beta:0', 'transformer/layer_1/intermediate/kernel:0', 'transformer/layer_1/intermediate/bias:0', 'transformer/layer_1/output/kernel:0', 'transformer/layer_1/output/bias:0', 'transformer/layer_1/output_layer_norm/gamma:0', 'transformer/layer_1/output_layer_norm/beta:0', 'transformer/layer_2/self_attention/query/kernel:0', 'transformer/layer_2/self_attention/query/bias:0', 'transformer/layer_2/self_attention/key/kernel:0', 'transformer/layer_2/self_attention/key/bias:0', 'transformer/layer_2/self_attention/value/kernel:0', 'transformer/layer_2/self_attention/value/bias:0', 'transformer/layer_2/self_attention/attention_output/kernel:0', 'transformer/layer_2/self_attention/attention_output/bias:0', 'transformer/layer_2/self_attention_layer_norm/gamma:0', 'transformer/layer_2/self_attention_layer_norm/beta:0', 'transformer/layer_2/intermediate/kernel:0', 'transformer/layer_2/intermediate/bias:0', 'transformer/layer_2/output/kernel:0', 'transformer/layer_2/output/bias:0', 'transformer/layer_2/output_layer_norm/gamma:0', 'transformer/layer_2/output_layer_norm/beta:0', 'transformer/layer_3/self_attention/query/kernel:0', 'transformer/layer_3/self_attention/query/bias:0', 'transformer/layer_3/self_attention/key/kernel:0', 'transformer/layer_3/self_attention/key/bias:0', 'transformer/layer_3/self_attention/value/kernel:0', 'transformer/layer_3/self_attention/value/bias:0', 'transformer/layer_3/self_attention/attention_output/kernel:0', 'transformer/layer_3/self_attention/attention_output/bias:0', 'transformer/layer_3/self_attention_layer_norm/gamma:0', 'transformer/layer_3/self_attention_layer_norm/beta:0', 'transformer/layer_3/intermediate/kernel:0', 'transformer/layer_3/intermediate/bias:0', 'transformer/layer_3/output/kernel:0', 'transformer/layer_3/output/bias:0', 'transformer/layer_3/output_layer_norm/gamma:0', 'transformer/layer_3/output_layer_norm/beta:0', 'transformer/layer_4/self_attention/query/kernel:0', 'transformer/layer_4/self_attention/query/bias:0', 'transformer/layer_4/self_attention/key/kernel:0', 'transformer/layer_4/self_attention/key/bias:0', 'transformer/layer_4/self_attention/value/kernel:0', 'transformer/layer_4/self_attention/value/bias:0', 'transformer/layer_4/self_attention/attention_output/kernel:0', 'transformer/layer_4/self_attention/attention_output/bias:0', 'transformer/layer_4/self_attention_layer_norm/gamma:0', 'transformer/layer_4/self_attention_layer_norm/beta:0', 'transformer/layer_4/intermediate/kernel:0', 'transformer/layer_4/intermediate/bias:0', 'transformer/layer_4/output/kernel:0', 'transformer/layer_4/output/bias:0', 'transformer/layer_4/output_layer_norm/gamma:0', 'transformer/layer_4/output_layer_norm/beta:0', 'transformer/layer_5/self_attention/query/kernel:0', 'transformer/layer_5/self_attention/query/bias:0', 'transformer/layer_5/self_attention/key/kernel:0', 'transformer/layer_5/self_attention/key/bias:0', 'transformer/layer_5/self_attention/value/kernel:0', 'transformer/layer_5/self_attention/value/bias:0', 'transformer/layer_5/self_attention/attention_output/kernel:0', 'transformer/layer_5/self_attention/attention_output/bias:0', 'transformer/layer_5/self_attention_layer_norm/gamma:0', 'transformer/layer_5/self_attention_layer_norm/beta:0', 'transformer/layer_5/intermediate/kernel:0', 'transformer/layer_5/intermediate/bias:0', 'transformer/layer_5/output/kernel:0', 'transformer/layer_5/output/bias:0', 'transformer/layer_5/output_layer_norm/gamma:0', 'transformer/layer_5/output_layer_norm/beta:0', 'transformer/layer_6/self_attention/query/kernel:0', 'transformer/layer_6/self_attention/query/bias:0', 'transformer/layer_6/self_attention/key/kernel:0', 'transformer/layer_6/self_attention/key/bias:0', 'transformer/layer_6/self_attention/value/kernel:0', 'transformer/layer_6/self_attention/value/bias:0', 'transformer/layer_6/self_attention/attention_output/kernel:0', 'transformer/layer_6/self_attention/attention_output/bias:0', 'transformer/layer_6/self_attention_layer_norm/gamma:0', 'transformer/layer_6/self_attention_layer_norm/beta:0', 'transformer/layer_6/intermediate/kernel:0', 'transformer/layer_6/intermediate/bias:0', 'transformer/layer_6/output/kernel:0', 'transformer/layer_6/output/bias:0', 'transformer/layer_6/output_layer_norm/gamma:0', 'transformer/layer_6/output_layer_norm/beta:0', 'transformer/layer_7/self_attention/query/kernel:0', 'transformer/layer_7/self_attention/query/bias:0', 'transformer/layer_7/self_attention/key/kernel:0', 'transformer/layer_7/self_attention/key/bias:0', 'transformer/layer_7/self_attention/value/kernel:0', 'transformer/layer_7/self_attention/value/bias:0', 'transformer/layer_7/self_attention/attention_output/kernel:0', 'transformer/layer_7/self_attention/attention_output/bias:0', 'transformer/layer_7/self_attention_layer_norm/gamma:0', 'transformer/layer_7/self_attention_layer_norm/beta:0', 'transformer/layer_7/intermediate/kernel:0', 'transformer/layer_7/intermediate/bias:0', 'transformer/layer_7/output/kernel:0', 'transformer/layer_7/output/bias:0', 'transformer/layer_7/output_layer_norm/gamma:0', 'transformer/layer_7/output_layer_norm/beta:0', 'transformer/layer_8/self_attention/query/kernel:0', 'transformer/layer_8/self_attention/query/bias:0', 'transformer/layer_8/self_attention/key/kernel:0', 'transformer/layer_8/self_attention/key/bias:0', 'transformer/layer_8/self_attention/value/kernel:0', 'transformer/layer_8/self_attention/value/bias:0', 'transformer/layer_8/self_attention/attention_output/kernel:0', 'transformer/layer_8/self_attention/attention_output/bias:0', 'transformer/layer_8/self_attention_layer_norm/gamma:0', 'transformer/layer_8/self_attention_layer_norm/beta:0', 'transformer/layer_8/intermediate/kernel:0', 'transformer/layer_8/intermediate/bias:0', 'transformer/layer_8/output/kernel:0', 'transformer/layer_8/output/bias:0', 'transformer/layer_8/output_layer_norm/gamma:0', 'transformer/layer_8/output_layer_norm/beta:0', 'transformer/layer_9/self_attention/query/kernel:0', 'transformer/layer_9/self_attention/query/bias:0', 'transformer/layer_9/self_attention/key/kernel:0', 'transformer/layer_9/self_attention/key/bias:0', 'transformer/layer_9/self_attention/value/kernel:0', 'transformer/layer_9/self_attention/value/bias:0', 'transformer/layer_9/self_attention/attention_output/kernel:0', 'transformer/layer_9/self_attention/attention_output/bias:0', 'transformer/layer_9/self_attention_layer_norm/gamma:0', 'transformer/layer_9/self_attention_layer_norm/beta:0', 'transformer/layer_9/intermediate/kernel:0', 'transformer/layer_9/intermediate/bias:0', 'transformer/layer_9/output/kernel:0', 'transformer/layer_9/output/bias:0', 'transformer/layer_9/output_layer_norm/gamma:0', 'transformer/layer_9/output_layer_norm/beta:0', 'transformer/layer_10/self_attention/query/kernel:0', 'transformer/layer_10/self_attention/query/bias:0', 'transformer/layer_10/self_attention/key/kernel:0', 'transformer/layer_10/self_attention/key/bias:0', 'transformer/layer_10/self_attention/value/kernel:0', 'transformer/layer_10/self_attention/value/bias:0', 'transformer/layer_10/self_attention/attention_output/kernel:0', 'transformer/layer_10/self_attention/attention_output/bias:0', 'transformer/layer_10/self_attention_layer_norm/gamma:0', 'transformer/layer_10/self_attention_layer_norm/beta:0', 'transformer/layer_10/intermediate/kernel:0', 'transformer/layer_10/intermediate/bias:0', 'transformer/layer_10/output/kernel:0', 'transformer/layer_10/output/bias:0', 'transformer/layer_10/output_layer_norm/gamma:0', 'transformer/layer_10/output_layer_norm/beta:0', 'transformer/layer_11/self_attention/query/kernel:0', 'transformer/layer_11/self_attention/query/bias:0', 'transformer/layer_11/self_attention/key/kernel:0', 'transformer/layer_11/self_attention/key/bias:0', 'transformer/layer_11/self_attention/value/kernel:0', 'transformer/layer_11/self_attention/value/bias:0', 'transformer/layer_11/self_attention/attention_output/kernel:0', 'transformer/layer_11/self_attention/attention_output/bias:0', 'transformer/layer_11/self_attention_layer_norm/gamma:0', 'transformer/layer_11/self_attention_layer_norm/beta:0', 'transformer/layer_11/intermediate/kernel:0', 'transformer/layer_11/intermediate/bias:0', 'transformer/layer_11/output/kernel:0', 'transformer/layer_11/output/bias:0', 'transformer/layer_11/output_layer_norm/gamma:0', 'transformer/layer_11/output_layer_norm/beta:0', 'pooler_transform/kernel:0', 'pooler_transform/bias:0', 'predictions/transform/logits/kernel:0', 'predictions/transform/logits/bias:0', 'cls/predictions/output_bias/bias:0', 'cls/predictions/transform/dense/kernel:0', 'cls/predictions/transform/dense/bias:0', 'cls/predictions/transform/LayerNorm/gamma:0', 'cls/predictions/transform/LayerNorm/beta:0'].\n"
     ]
    }
   ],
   "source": [
    "def get_loss_fn():\n",
    "  \"\"\"Returns loss function for BERT pretraining.\"\"\"\n",
    "\n",
    "  def _bert_pretrain_loss_fn(unused_labels, losses, **unused_args):\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "  return _bert_pretrain_loss_fn\n",
    "\n",
    "keras_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=get_loss_fn(),\n",
    ")\n",
    "\n",
    "keras_model.fit(\n",
    "      pretrain_ds,\n",
    "      validation_data=(pretrain_ds),\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs, \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We have 25000 examples for training\n",
    "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# We have 25000 examples for testing\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "    config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ").batch(config.BATCH_SIZE)\n",
    "\n",
    "# Prepare data for masked language model\n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_all_review\n",
    ")\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    \n",
    "    \n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {'attention_probs_dropout_prob': 0.01, \n",
    "               'hidden_act': 'gelu', \n",
    "               'hidden_dropout_prob': 0.1, \n",
    "               'hidden_size': 768, \n",
    "               'initializer_range': 0.02, \n",
    "               'intermediate_size': 3072, \n",
    "               'max_position_embeddings': 128, \n",
    "               'num_attention_heads': 12, \n",
    "               'num_hidden_layers': 12, \n",
    "               'type_vocab_size': 2, \n",
    "               'vocab_size': 22001}\n",
    "\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model, transformer_encoder, pretrainer_model = bert.bert_models.pretrain_model(\n",
    "    bert_config, seq_length=128, max_predictions_per_seq=2, use_next_sentence_label=False, return_core_pretrainer_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABnMAAAC7CAIAAAAxL/wrAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdeVwT1/4//gOygyyy1mKBACqLAhVQECutUsW9VUSr1tbla6u1eq1We7kW63K1VRF7VWq5Lr0XW9SCSvF6KwoieNtaFlEJmyCrC2DZUQJkfn/M5zePdAIhZJsQXs8/fCTD5Mx7knnlZI6zaFEURQAAAAAAAAAAAKCftLkuAAAAAAAAAAAAYEDCyBoAAAAAAAAAAIAsdJhHGRkZHR0dHJYCoFZ8fX3Nzc2lnPnXX39tbW1Vaj0AnDAxMZkwYYKUMzc2NmZlZSm1HgD1N2LEiFGjRkk588OHD0tLS5VaD4DGCwgIMDY2lnJm7PLAYIOdGgDlYTogLfo6a3V1dV5eXrNnz+a6MOjVtWvXpk6dynUVpKOj4/bt25MmTeK6EOXKz89fsmTJhx9+KOX8lpaWCxYsUGpJQKuurm5paXFzc+O6EFJQUDB06FB7e3uuC1GuH3/88dmzZ1LOHBMTc+bMGQ8PD6WWBP11584de3t7KysrrgshGRkZ/v7++vr6XBeiRM+fP6+urk5NTZVy/vDwcIqiLCwslFoVqBv0ZQqUnZ29devWsLAwaWbGLs+AUF9fX11d7e3tzXUhpKysjBDC4/G4LkR22KkBFnRACiTaAf3fMWsURY0aNer48eOcFgaSeHh4qMMHVFtbGx4erg6VKNXRo0eFQqH089vZ2Wn8e6Imzp07x+fzd+zYwXUhZMeOHe7u7gsXLuS6EOXKzMyUfmahULh48eJ169Yprx6Qwbp168LCwoKDg7kuhLz++utfffWVjY0N14UoEd1LSj8/RVGRkZEYjx5s0Jcp0I4dO6S/IRt2eQaEGzdunD9//ujRo1wXQugaBvQPG+zUAAs6IAUS7YBwnTUAAAAAAAAAAABZYGQNAAAAAAAAAABAFhhZAwAAAAAAAAAAkAVG1jREZ2cnj8fr7OyU7eUlJSXy12BiYpKQkPDgwQP66b1794qLiy9evGhhYREZGUlPLCgomDhx4nvvvdfQ0CD/Euvr6z/99NNDhw7RTymK2rJlC4/H8/T0vH37NiFk//79J06c2L59e3x8PCEkKyuruLiYnvnu3bsxMTGurq7ylwFqRd2yoIIgkD9nQTwIpPcsIAigbpEh6tF9kD+nBt0HqJi6BVMFqWTFEKEDCRAQBASUB/mSLV8YWdMQurq6t27d0tXVleG12dnZq1atkr8GCwuL+fPnu7i4EEIyMjLKy8tHjhw5b9680NDQL7/88r///S8hxM3NLTw8fMOGDQq5D1p3d3dnZ2d7ezv9NCEhYeHChWVlZUuXLl27dm1ubu6vv/66cuXKHTt2fPXVV4QQX1/f69evP378mBAyduzYFStWyF8DqBu1yoJqgkD+nAVWEAghErKAIIBaRYaoR/dBxFKD7gNUTK2CqZpUsmKI0IEECAgCAsqDfMmWL4ysaY7a2tqOjo7z588HBgaeOHEiMDDw4MGDhJBTp04FBwfv3LnT19d3zZo10dHRfn5+hJCoqCh68DUnJ6eysjIuLk4oFDo7O1dVVclZiVAo3L9/P3NHc2dn56ioqKVLl1ZWVhJCzMzMTE1N6QLOnTu3YcOGuLg48bIJIdHR0Xv27Jk+ffrTp097XJCtra2JiQnz1N/fn161JUuWPH/+3MbGJiMjo6CgID8/f/z48fQ8ixcv3rhxo5wrCGquxyywgkAIUXYWZAgCIUQ8C30Ggfw5C6wgEEKQBZCMjgwR2/z6TA0TGULIwEqN5O6D9JQaRAZUbLD1ZeIxROhAAg0LCOkrIwgIqJKG5Us1HRBG1jREZmamn59fa2trUFAQn88PCws7c+bM6dOnCSETJ07k8/mbN29OT0+/cOHCkCFD6Je8/fbb9AN/f38rK6ulS5dqa2snJiba29vLWUxWVpahoaHolLVr186aNSssLEwgENBTEhMTS0pKFi5cuH///q1bt1pZWbHKPnv2rJOTU0RExPDhw+mQ9OmVV16hH+Tk5Lz11lsvv/zy4cOHg4KCDh8+fOzYMfpP5ubm6enpTU1Ncq4jqK3essAKQlpa2tSpU+mXKCkLMgShsLCQVbb8QSCEIAsgARMZQkh/U8NEhhCi8alBZECVBmFfJh5DhA56o2EBIVLs+CAgoDIali+VdUAYWdMQQUFBw4YNI4To6ekZGhqampoOHz68urqaEKKvr29paWlkZGRsbBwQEJCXl0e/REtLS7wdLy+vHqf3C5/Pt7S0ZE385ptvuru7N23aRD+9evXq2LFj6YIDAwMzMzNZZaekpNy9e/e7776bPHmyr6+v9EsXCoXXr1/fvn07IaS9vT0iIuLKlSvMaAIhxMXFhc/ny7mOoLZ6ywIrCHfu3GE2dSVlQYYgpKamsspWSBAIsgC9YyJDkJq+UoPIgMoM2r6MFUOEDnqkYQEhUu/4ICCgAhqWL5V1QDr9WDNQb9JsuHZ2doq6rpMEOjo6zc3NrIkGBgYJCQm+vr6lpaVBQUGEkNraWvpPRkZGBgYGrPkbGhpCQ0Pnz59PCKFPU5LS6dOnP/nkE319/Tt37qSlpcXFxc2dOzcgIGD58uX0uT8mJiZCoVDmtQP112cW7OzsrKyslF2GmgSBEIIsgGRSdh9IDSIDqjQ4+zLRGBL0U9A7TQoIkTojCAiohiblS2UdEI5Z0xzU/49+2t3dzTxmton8/PywsLD6+npCSEVFBX0UpaGhIX0eEEVRfD6feZXMPD09mVu8EUJqamroBw4ODvHx8SkpKYSQGTNm/Pzzz/T0goIC5lBSpmwfH5+YmJiurq6Ojo7k5GShUNjjedGsai9fvhwUFOTg4CAUCtPS0lpaWgghzs7Ojo6ObW1t9Dz19fVubm5yriOos96yIBqEN99808zMTKlZkCEIISEhrLKlDAL5cxZEg3D//v26ujpkASRg8tLf1IhGhhAysFIjofvoLTWIDKjSIOzLWDEkCB30TpMCQqTb8UFAQGU0KV8q64BwzJqGyMjIqK2tvXTpkrGxcUNDQ15eXlFRUXNzc1ZWlrW19ZMnT2JjY7u6ujZv3jxu3LigoKCQkJA5c+aYmprm5uZ6eXkZGxtHRER8+umnM2fOTE9PZ840lo2Xl5elpSVFUVpaWomJiYmJiTNmzFiwYAEhZMqUKXv37iWEzJkzJy8vb9++fSNGjNi+fXtRURGr7PXr1//vf/9zdnaeMGHCiRMnfvnllylTpvD5fB6PxyyopKTkxo0blpaW5eXljo6OFy9eXLlyJX0hw7a2ttzc3KqqKnrF3333XVtbW0JIS0uLq6src+oTaB4JWRANAr09KDULMgRhxIgR8fHxomUHBAT0GQTy5yzcuXNHNAh37tzx8PC4cuVKb1no10E9oHmYyKxcufLatWv9Sk1LSwsdmW3bthkbGw+g1EjuPnpMDboPUKVB2JeJxxChg95oWECk2fFBQEBlNCxfquuA6MHIp0+fBgcHUwNBfn5+dHS0YuccENzd3WV7YXl5+ejRoxVVhoRNxd7ennl89+7d8+fPK2qhtMTExMbGRjkbOXbsWGlpKf34xYsXLi4uPc525MiRI0eOSN+szJ+OKiUnJ//lL3/hugp5nT17NjIyUoYXKjYIFEVFRkaePXu2xz8xWVDbIFAiWZAQBKqf23Z/g6PONKkHWbt2bVpamgwvVHhqgoODnz59Kj5dk7qP/v6gCgsLu3//vpy1MaT/ntekLXwgQl8mTSqlDJ2EVRCn8F0eKUOngsRpUqjT0tLWrl0rwwsVHhAJP2yUGhBKiowMiJ0a9QmI9HJzc7/99luuq1AidEBK6oD6OBu0s7OTx+N1dnbKMMRICCkpKZHthRIIhcIdO3aITxcvtbc5Wa5fv25vb+/j4/P777/LVhK9mvRtN+fMmVNRUSFbO0rS2trKnPylVM+ePUtISCgrKyOEjBkzxsXFpbCwUFGNCwQCV1dXMzMzeRrJzc0NDQ2lx6fz8/Pj4+MVVF3P1C0+Pj4+p06dkjCDZmdBZUEgIllQzyAQkSyoIAhEaVlQVEZ6LE81PQizCoM8Neg+FKXP73lGb1t4enr6uHHjHj16pODK/kw0dHQK5PyWEIW+TFG47cs0LHQS+hTZQocdnx5pRkCIFBlBQOTXW7/z6NGjr7/+mjVRyt+c+FmoKAOxA+pjZE1XV/fWrVu6urrS18rIzs5etWqVDC+UzNXVtcfp4qX2NifLlClT/P39Q0JC/Pz8ZKiHWc233nrLwcFh0aJFDg4OMrSjPCkpKeHh4deuXVP2gtrb2+fPn88cV+nt7T169GhFNa6np+fp6SlnIz4+Po6OjvRjDw+P5cuXK2Pwl6Fu8elzx1Kzs6CyIJA/Z0ENg0BEsqCCIBDlZEGBGemxPBX0IKKrMMhTg+5DUaQfQOxtC588efKzZ88oua+4KhkTOiYF8nxLsKAvUxRu+zINC52EPkW20GHHp0eaERAiRUYQEPn11u84Ozuzpkj/mxM/CxVlIHZAfd/BoLa2tqOj4/z584GBgSdOnAgMDDx48CAh5NSpU8HBwTt37vT19V2zZk10dDS9AUVFRdHZyMnJqaysjIuLE2/z8OHD9vb29fX1f/3rX3k83qNHj+rq6l577bV9+/adO3duw4YN9KtOnjwZEhKyc+fOSZMmEUKOHDly5swZ+nxaCaWKz9ne3n7o0KEzZ87s2LFDKBQ6OztXVVWJvnDIkCFDhgwhhMi5mkw7jJaWlsjIyKSkpHfeeWfv3r3GxsZlZWUURS1btiwnJyc6OnrPnj3Tp09/+vQpa2UVaOPGjfv372duEQCqpIz4JCcnjx07NjY2dt68eevWrfvpp59WrFgRGhpK/ry9CQQC0S1ftIW9e/euWbOGz+cPqiwgCNzqMQusjYoQIn0WRKcfPHhQdItav349q1m6ZWYb6608+rFoD8IKUb96kD7XjrVqrNSw4sxaR6RGs0n4nifSfdXT3/P0//GyNv4+f03RuxmSa2AR39oJIVFRUcyPOvEi6dCJpoCJoegLiViylJpKgr5ssFLUj6vCwsL+Jo7IFDrSy44PAoKAKMOAC0hvow1fffWV6NKZEH3zzTfffPPNjBkzIiIifv311+7u7kOHDgUEBOzevZuIbPb4WYh8SUKfFNrbRQcyMjJ0dXXr6+sfPXpkZmbW1NRUVlbm6elJUVRRUZG1tXVbW1tra6u1tfXXX3/t6+tLUdTDhw/p01Dv3LlDTxH34sULGxubhoaG1tZWS0vLjo6O+vr6OXPmfPDBBxRFdXR0DB8+vKCgoLCw0NTUtK6u7uHDh6mpqfRfKysrzc3NJZQqPueFCxf2799PUdTly5fpwoRCoehrFyxYsG3bNoqi5FxNFxeX+Ph40ZbT0tI2btxIUdT8+fNzcnLs7e3pe2R88cUX8fHxFy9epCjq/fffP3DggOjK9vimqcmVvAbQJfnkoZBLEigpPgKBQFtb+48//uju7jYyMsrNzaUoatSoUXV1daLbW15eHmvLb21tNTc3LygoiI2NpZsaiFmQ+dIACtevC7sMXAq5zlpvWWBtVKmpqffu3ZMyC6LTu7q6RLco8WZZ21hv5VEUxepBWCGi+pOaPteOtWqs1LDizFrHfqVG5uusKVxv11nTJAq5zpqE73lKbNsQ3UrFv+dZm0qfv6YoinJxcamurpZcA4v41p6QkCD6o27v3r2iUWJCx6SAmcJ6YUFBAStZSk0lhb5sAFLIddYU9eNKhsRRMoWutx0fzQiIzNdZUzgNuICsQnZqBlxAehxtCA0NFV06E6K6ujpvb2+Koo4dO7Zy5crCwsLhw4e3tLRUVFTQlxVjNnvN+FmIDkiBRFehj3uDBgUF0TdE0NPTMzQ0NDU11dfXr66uJoTo6+tbWloaGRkRQgICAvLy8uiXaGlp9Tmcp6+vP3v27AsXLvj4+FhYWFy9erW2ttbIyGjs2LH0sgIDA1NTU2fOnGljY2NlZWVlZXXw4MHx48cTQmxsbCSXmpiYyJrT399/7dq1jY2NkZGRhBAvL6/eClPsahJCgoOD/fz8oqOjc3JyamtrV6xYceLEib/97W+2trYpKSkODg6NjY2TJ092dHQ0MDBgVrbHpiiKamhokGahStXY2NjZ2akOlShVe3s7/aHLQ0nx0dXV1dbWtrCwIIRYW1tbW1sTQuzs7EpKSkS3t8ePH7O2fEJIV1fX8uXLU1NT6acDNAsvXrxQhy3wxYsXbW1t6lCJUjF315ZHb1lgbVR37tx588036ZdIuV3RhgwZIrpFiTebn58vuo31Vh4R60HEQyR9auRcO1acx44dK3NqKIpqaWlRh221q6ursbFRIWf8qa3Gxkb5UyPhe97KykrCV71AIGB9z7M2lR9//FHyrykpa2DNLL61FxUV0WGhf9S1traePHmSiZJo6GjMlKtXr4q+MDU1NTw8XDRZHKaSoC9TSy9evJC/EUX9uJI5cZJrEN+Ketvx0ZiACAQCddgs29vbCSHqUInMFLJTM+AC0uNog7Gx8d27d5mlMyF68eLFw4cPhULhmDFj0tPTCSHDhg0zMTExMTEpLy8XbVYzfhYSdECKI9oB9TGyRqT7pO3s7OitXHrvvPPOV199VVdXd+jQoXPnznl7e5uZmdXW1tJ/NTIyMjAwEJ2fHp+WplTxOe3s7DIzM5csWZKfn3/hwoV+1clqp1+r2dnZ+eTJk2XLlh06dOi3334jhKxYsWLChAleXl6zZs26evVqaGjo/PnzCSEdHR1PnjyR3Fpzc/PChQtlLl5RBAJBaWmpOlSiVFVVVevXr5e/HSXFR7x9LS0toVBYVVUlur2Jb/k6OjpTp07dsmXLsWPHZFuibDUrNgtJSUnZ2dny1K8QVVVVenp633//PdeFKFdjY6NC2ukzC3Z2dr11/9IQ3aK6urpYzTY0NIhuYxLKY/UgCuw++rt2ZWVlK1asYCJD5EhNa2vrF198IfP3jAKVlpauXr1aT0+P60KUSCAQNDc3K7BB1vc8IUTCV31cXBzre5618X/77bd9/pqSpgYJ6K29qKhI9Eedo6MjK0ri3wnMFAm/BgmnqURfpp5qampeffVVBTYoz48rhSROvAYJ86hPt6XYgNy6dUsddjfoE/2uXr3KdSGyU9RODWOgBER8tEEoFLq7u4v+GqTbsbe3X7169a5du2xsbCIiIiQsVzN+FhJ0QIoj2gH1PbJGH9vGPO3u7maeMttxfn7+4cOHz549SwipqKgQCASEEENDw9bWVroF8d9PwcHB77///ty5c6dNm7Z27dr58+e7uLjs27ePHgAuKCjYvXu36LK8vb0vXry4cuXK9vZ2uv3eShWfMz4+fu7cuenp6X5+fhRFFRQUuLm5iZbU3d1Np5pZnGyrKfrVQFFURESEhYWFn5+fj49PfX19d3e3g4MDXd6yZct8fHxiYmLmzp3b3d2dnJw8btw4SuKlGc3MzFJSUiTMoBq1tbXh4eHqUIlSHT16VCHtKCk+ou3T7dDNxsXFiW5vrC2fnmf79u3e3t5Xr1598803+Xz+QMzCwoULpbn7lbLt2LHD3d1dHX72KZWHh4dC2uktC6Ib1c6dOzs7O+vr64kUWWBNF92iKioqWM1WVFSIbmP0L48ey2P1IKwQaWlp9Ss1kteOtQqs1Dg7O2/evJmJDCFE5tQMHTr0wIEDwcHBUn5YyvP666+fPXtWmv+aHrjoXlKBDbK+54nEr3p6TtHvedamIs2vKWlqEMfa2s3MzER/1Lm7u9va2opGiQ6daApoM2bMYP0aZCVLqakk6MsGIIW/jfL8uFJI4sRr6G0e8R0fjQnI66+/rqhf4/Kga1i3bh3XhchO4W/jQAmI+GhDe3s769cgU6GNjc2mTZvo65oVFBTQjQuFQvoBs9n/8MMP8+bNG+g/Cwk6IMURfRv7GFnLyMiora29dOmSsbFxQ0NDXl5eUVFRc3NzVlaWtbX1kydPYmNju7q6Nm/ePG7cuKCgoJCQkDlz5piamubm5np5eRkbG0dERGzbtm3o0KGslrW1tdeuXbtw4UJdXd0PP/wwNDRUT08vLy9v3759I0aM2L59+4gRI77++uvKysobN24EBwd/+OGHGRkZb7/99quvvsrj8ejfiz2WKj5nd3f3li1bZs2a9fHHH1MUNXPmzPT09FdeeYV+4c2bN7Oysh4+fJiTk1NcXCzzanp4eFRUVJw+fbqysvLZs2fp6emTJk2aNGnSypUrhw4dOnLkyOPHjwcFBS1fvpz+34/169f/73//c3Z2njBhwokTJ06ePMmsrNwfMagFJcUnOTm5q6vr1q1bJiYmjx49unjx4qxZsx48eHDz5s2goKBVq1Yx29ubb77JbPlaWlqXLl1qbW29f//+pk2bFi1adOrUqY0bNyILoAISsiC6Udna2hJCpMyCi4sLazqzRRFCWM2ytrHeylu5ciWrB7ly5UpmZiYTIqFQ2K8eRPLatbS0MKtw48YNVmoCAgKSkpJEI2NqaorUDBISvucDAwNZX6eiX/VJSUmi3/Pfffed+MYv+dfU7du3q6urL1++PHz4cAk1iP+XD2trnzNnjuiPusbGRtH+iAnd+++/T6dg4sSJTAxZvwbj4+NFk/Xbb78pL5XoywYtRf24+vrrr/X09KRPHJE1dL3t+Ci120JABq0BFxDS02hDcHCw6EbIhCgsLOzo0aPR0dHm5uZBQUFOTk4VFRX37t0rLS0VCAQZGRkTJ06kN/uXX35ZdO3wsxD+hB6pleGy9OXl5fQl/TSbwlfz3//+d2/3KJAMdzBQJYVc7FOCgRgfNckCLrqpYgq5g0FvFLtRMVuUmuRLGWXIlhrcwUCVFHIHgwFHTULXJ/VJJfoyBVLIHQyAUqeA4A4GCqTsnRoNkJub+/3333d0dFRUVPz0009XrlxRxlLUJ1/ogBSoH3cwkKC1tbWtrU2aOdetWyd6/rO9vb3kE5i5arNH0q9mn2prawUCQV1dnfj1s2Gw4TY+skEWQOH6tVFJyAJri+qzWdXESoGRIUgNKIfMWWC9sKmpSYFbu/IglcA52UKHbgsGCU4CEhMTM3z48NbWVvr0zNDQ0H7VLCXkS+PJPrKWkpISHh5+7dq1qVOnSp5TGSfJq+zEe+lXs08nTpyIiYm5cuWKQgqTXklJiaurqzJmZjExMfnuu++8vLxcXFwIIffu3dPX1+fz+e+///7HH3/8xRdfEEIKCgpWrVrl6up66NAh+S+nXV9f/9VXX7300kt/+ctfCCEURX366acJCQlGRkYnT5709/ffv3//sGHDysvLPTw8Fi1alJWVZWpqOnLkSELI3bt3b926FRUVVVJSImcZMuA2PrIZVFmQJwjkz1lQQRDIn7MgHgRCSG9ZGChBIBKzwNqi+mxWNbFSYGSIpqdGDbsP8ufUuLi4qEn3oVgyZ4H1wujo6JdffllRW7vyIJX9ouK+jBVDbW1thE7OV/UXAtIvCIgycBKQL7744tSpU5988om7u/vq1avNzc3laa03yFe/DMh80Yeu4dBo9SfboblZWVmvvfaaAmeWsKnY29szj2/evJmUlEQ/Xrx4sb6+PnNg7eHDh3NycqQsSbInT55s3LiRvrwxRVHnz5+/ffs2RVF79+4dN25cTk7O22+/TVFUV1eXj48PPc+xY8cePXpEP37x4oWLi0uPLePAabUl8wHM0mdByjklHMDMZEE1QaD+nAVWECiKkpwFCUGglHw2KKiGzGeDKjw1vZ0Nqm7dB9VTaqTsPgbn2aDQX+jLxInHUMrQ4WxQzSPz2aAKD4iEHzYDJSDYqQEWdEDiFNIBacs5vAeci4qKOnfu3IYNG+Li4ggh0dHRfn5+9HRXV9ecnJzKykr6T6dOnQoODt65c6evr++aNWtYcxJCmJmFQqGzszNzLfD+EgqF+/fvnz17Nv3U2dk5Kipq6dKllZWVhBAzMzNTU1NW5efPnw8MDDxx4kRgYODBgwfpF0ZHR+/Zs2f69OlPnz7tcUG2trYmJibMU39/f3qNlixZ8vz5cxsbm4yMjIKCgvz8/PHjx9PzLF68eOPGjbKtF6g50S1KwubNCgLpPTWqDwIhRDwLfQaB/DkLrCAQQpAF6M2gTY3k7oP0lBpEBlRj0KZSPIYIHYhTeEAIIfJkRFEBIX1lBAEBFUAHJE++MLI2sCUmJpaUlCxcuHD//v1bt24tLCxkji99++23CSH+/v5WVlZLly4lhEycOJHP52/evDk9Pf3ChQv0fYWZOUVn1tbWTkxMtLe3l62qrKwsQ0ND0Slr166dNWtWWFgYc0NlVuVWVlZ8Pj8sLOzMmTOnT58mhJw9e9bJySkiImL48OF0SPrE3JYlJyfnrbfeevnllw8fPhwUFHT48OFjx47RfzI3N09PT29qapJt1UBtsbYoZusV37xZQUhLS+stNaoPQmFhYVBQkGgW5A8CIQRZgB4hNQxpUoPIgAoM5lSKxxChAxZlBIQQIk9GFBIQIsWODwICyoYOiMiXL4ysDWxXr14dO3YsIURPTy8wMDA1NZW55bD4vYf19fUtLS2NjIyMjY0DAgLy8vJ6m5MQ4uXl1eN0afD5fEtLS9bEb775pru7e9OmTT1WnpmZaWhoaGpqOnz48OrqakJISkrK3bt3v/vuu8mTJ/v6+kq/dKFQeP369e3btxNC2tvbIyIirly5wowmEEJcXFz4fL5sqwZqi7VFpaWl0dP7DMKdO3ckpEbFQUhNTdXT0xPNgkKCQJAF6AlSw9JnahAZUDakkhVDhA5EKSkgRI6MKCQgROodHwQElAcdkJz5kv0OBqAmamtr6QdGRkYGBgZSvsrOzk4h103vkY6OTnNzM2uigYFBQkKCr69vaWlpUFAQ6avyhoaG0NDQ+fPnE0I6OjqkX/rp06c/+eQTfX39O3fupKWlxcXFzZ07NyAgYPny5fS5PyYmJqK3jwGNIUMW7OzsrKyslFSPmgSBEIIsQG+QGlF9pgaRARUY5KkUjdYaz+0AACAASURBVCFBPwViNDIgROqMICCgVBqZL5V1QDhmbWCbMWPGzz//TD8uKCgICQkxMzOrr68nhFRUVAgEAkNDw9bWVkIIRVGEEGbjyM/PDwsLE52TECI6M5/Pp18iA09PzwcPHjBPa2pq6AcODg7x8fEpKSnilTNHkHZ3d9PL9fHxiYmJ6erq6ujoSE5OFgqFPZ4XzSry8uXLQUFBDg4OQqEwLS2tpaWFEOLs7Ozo6Mjc57i+vt7NzU22VQO1xdqiFixYIGHzFg3Cm2++KSE1Kg5CSEgIszg6C1IGgfw5C6JBuH//fl1dHbIA4gZ5aiR0H72lBpEBZRvkqWTFkCB08GdKCgghROaMKCQgRLodHwQElAodkJz5wsjawDZnzpzQ0NB9+/adOXNm+/btI0aMsLe3DwoKCgkJycvLMzU1bWlpMTY2joiIoDfuJ0+exMbGxsTEbN68edy4caJz5ubmuri40DM3NzfPnDlT5msNenl5WVpa0tt0YmJiYmLijz/+SP9pypQpe/fuFa+8qKiooaEhLy8vOTm5ubk5Kytr/fr1enp6zs7O77777rRp03755RcHB4eysjLRBZWUlNy4ceP27dvl5eWEkIsXL9IzOzk52dnZLVy40NnZmV7fd99919bWlhDS0tLi6uo6bNgw2d90UEusLSogIKC3zbu1tVU0CLa2tr2lRvVBGDFixLVr10SzEBAQ0GcQyJ+zwArCsGHDpk6diiyAuMGcGsndR4+pQWRABQZzKsVjiNABizIC0traKhQKZc6IQgIizY4PAgLKhg5I3nzRtwjFLajVn/y3QC4vLx89erScjUjYVJib41IUdffu3fPnz8u5LJbExMTGxkY5Gzl27FhpaSn9GDeoHohkvlE0QyFBoKS7UbTaBoESyYKEIFD93Lb7GxxQjbVr16alpcnTgqJSExwc/PTpU/HpmtR99PcHVVhY2P379+WsDQYc9GXSpFLK0ElYBXHY5RkQ0tLS1q5dK08LigqIhB82Sg0IJUVGsFMDskEHpKQOCNdZG0RaW1uZs8CU4dmzZwkJCT4+Pjweb8yYMd3d3YWFhaNHj1ZI4wKBwNXV1czMTJ5GcnNzQ0NDHR0dCSH5+flZWVkKqQ0GFmUHgYhkQT2DQESygCCANNB9oPsAdaPxfRlCB/IY0AEhUmQEAQEODeh8Ka8DwsjaIJKSkhIeHn7t2jXmomaK1d7eLvrU29tbgY3r6el5enrK2YiPjw/z2MPDw8PDY/ny5XK2CQOOsoNA/pwFNQwCEckCggDSQPeB7gPUjcb3ZQgdyGNAB4RIkREEBDg0oPOlvA4II2uDyMaNG7kuAYB7CAJAfyE1AOoGqQSQAAEBUB7kq0e4gwEAAAAAAAAAAIAsMLIGAAAAAAAAAAAgCy2Kogghzc3NHh4eenp6XNcDvWpqapL/suXyEwqFra2tpqamXBciyaNHj3R0dPT09HR1del/+9tCR0dHVFTUwoULpZyfx+NpaWn1dykgA4FA0N3dbWhoqJrFPX/+XCgUGhsb9/inIUOGaPzXJkVRrPtSS3Du3LlNmzbp6+srtaSBRSgU0t/e2tqc/VdWW1ubbN+ECtfc3GxiYsLhW6ECFEUFBAScOXNGyvk//vjjpKSkIUOGyLPQzs7Ozs5OgUBA/2tjY6MOHzdIoOK+TAIN6MueP3/+73//e8qUKdLMjF2eAYH+Kuvx15eKvXjxghBiYGDAdSGyw04NsKADUiDRDuj/RtYANElnZ2dxcTGfz8/Pz+fz+SUlJd3d3aNHj3Z3d/fw8HB3d3dzc9PsXTtQlKqqqgMHDmRmZm7btm3BggX4qQHSa2trO3LkyMmTJ1esWPHxxx+rwy8Y0ABdXV2VlZVlZWV0B5efn19bW+vg4EB3cDwez8vLy9ramusyAQAAAAYRjKzBoCAQCEpKSuidkOzs7NLS0iFDhvj4+NADbR4eHk5OThg0gd5UVFT8/e9///333z/77DOMr0Gf2tvbY2NjY2Njly1btn79eiMjI64rgoGKHkdjBtH4fH5jY6OTkxPzH0Xe3t4mJiZclwkAAAAwqGFkDQappqamBw8eMLsrNTU12trazL6Kr6/vSy+9xHWNoF4ePny4b9++/Pz8rVu3zp49m+tyQB0JBILTp09HRUXNmzfvs88+U4dT+GEAYQ64pg9JKy4ubmlp4fF4zH8Cubm5YaAWAAAAQN1gZA3g/zQ0NIgeF1BdXW1pacnsz4wdO9bGxobrGoF7fD5/37595eXlO3bseOONN7guB9RFZ2fnDz/8cODAgRkzZmzbts3c3JzrikDdiR5MTV+4oK2tbdSoUUy/4+7ujpOIAQAAANQfRtYAeiU61padnV1VVTVixIhx48bROzxeXl5Dhw7lukbgxv3793fu3Pn48eM9e/a89tprXJcDXBIKhQkJCbt37w4ODo6IiMAQPPRI9EDpsrKyBw8edHV1iV4AdPTo0XLexwAAAAAAOIGRNYB+ePToEXN8Aeu60e7u7q+++irO0xlUbt++vWvXrhcvXuzdu9fX15frckDV6DG1nTt3BgYG7tixA6eQA6OxsbG0tJTpLKqrq3V0dESvj4Yb6QAAAABoDIysAciFGWvLzs6mry3t5ubGnMvj4eExoG/UDdL45Zdftm/frqWl9dVXX/n4+HBdDqgCRVHJycm7du3y8fH5/PPPX375Za4rAi6xLibwxx9/aGlpMYNoHh4ejo6OGEcDAAAA0FQYWQNQJNZ93IqLi5ubm52dnen9q3HjxuF8H02VmZkZGRlpaWm5a9euUaNGcV0OKNG1a9f+9re/jRw5cseOHTwej+tyQNVY42g1NTXDhg3z8PDg8Xi42TQAAADAIISRNQDlYu71xlyjuru7W/TaOjgnSJPQYy6vvPLKnj17XF1duS4HFAyf7yAkehGAsrKyyspKa2tr0QOTcRYwAAAAwCCHkTUAVRO9H1x2dvaDBw90dHR8fHyYXTUc7zDQ4ZgmzYNjEgcJ1sU06+rqXnnlFeZgNNwkGgAAAADEYWQNgHvNzc0lJSXMvlxNTY22trboNXowOjPg4DpcGgPX0dNUrJP3+Xx+U1OTo6Mj892LG0ADAAAAgDQwsgagjpj7ytE3Rqiurra0tGQG2saMGWNra8t1jdA30XtHfvHFF3Z2dlxXBP2Ae79qks7OzqqqKtGLYLa0tPB4POb6aD4+PsbGxlyXCQAAAAADD0bWAAYG1jWzKyoqbGxsmBsj4NgKdUaPr+3evTs4ODgiIgJnk6m/+/fv79y58/Hjx3v27Hnttde4Lgf6TfSkez6fX1xc3N7ePmrUKOb/J9zd3Q0NDbkuEwAAAAA0AUbWAAYq1vWAamtrHRwcmJ1GHH+hbjo7O0+dOnXo0KG5c+du27bN3Nyc64qgB3w+f9++feXl5Tt27HjjjTe4LgekQp9QX1ZW1uONYng8nqenp76+PtdlAgAAAIBmwsgagOZgxtroc0gbGxvd3NxwjIZaEQgEp0+fjoqKmjdv3meffWZmZsZ1RfB/Hj58uG/fvvz8/K1bt86ePZvrcqBXzMny9NddaWnpkCFD3NzcmC+60aNHDxkyhOsyAQAAAGCwwMgagMZiXZ+7uLi4ubnZ2dmZGWvDcRxcaW9vj42NjY2NXbZs2fr1642MjLiuaFCrqKj4+9///vvvv3/22WcLFizAnXnVCnMiPH1I2qNHj7S0tERv8OLo6Kitrc11mQAAAAAweGFkDWAQYa7hTR/Uxjpnyt3d3c3NDfuoKtPW1nbkyJGTJ0+uWLFiw4YNBgYGXFc06FRVVR04cCAzM3Pbtm0YU1MHrAtK1tTUGBkZiY6jOTk54WMCAAAAALWCkTWAQY11ne8HDx50dnbS51WNGzcO+7Eq0NLScuzYsbi4uFWrVn3wwQc4ilA16urqDh48ePXq1Y0bNy5ZsgQnD3JC9GKRZWVllZWV1tbWzCCah4fHSy+9xHWNAAAAAAB9wMgaAPwJfS1w5rCR6upqHR0d0eu18Xg8rmvUQM+ePfvHP/6RkJCwfv36FStW6OjocF2Rxqqvrz9w4EBSUtLGjRvxVqsS66YrdXV1r7zyCo/Ho79Yxo4di9vmAgAAAMBAhJE1AOgDc71w+hzSmpqaYcOGMQNtY8aMsbW15bpGDUEP+vz0008bNmxYuXIlDqRSrD/++OPrr7/G8KUKsC7yyOfzm5qaHB0dmfM6vby8hg4dynWZAAAAAAAKgJE1AOg31rWQWOdweXt7W1lZcV3jAFZbWxsVFUWfqLh06VJc+U5+ra2tR48exSXtlIS5gCNzs5TW1lYnJyf6EFd3d3cfHx9jY2OuywQAAAAAUAqMrAGAArDO86qtrXVwcGCOT8F+tQwqKyv37Nlz+/btv/71r7i4vsza2tr++c9/0rdh/fjjjw0NDbmuaMBjXZyxuLi4vb191KhRzNi6u7s73mcAAAAAGDwwsgYASsEaa3vy5ImTkxNzYwTse0upvLx87969WVlZ27ZtCwsL47qcgUQgEJw+ffrrr79esGDBpk2bTE1Nua5oQKIvvFhWVkZnmXVDYR6P5+npidtuAAAAAMBghpE1AFAF1nWXiouLW1paeDwec5wL9s8lKCws/Pvf//7w4cMdO3ZMmTKF63LUHT2mFhUVNW/evM8++8zMzIzrigYM5qKKdE5LS0uHDBlC3yyYjuro0aNx+T8AAAAAAFEYWQMAbjDXZqJvjFBcXCwUCpljYdzd3d3c3HCJMVH5+flffPHF48ePd+3aFRwczEw/fvw4j8cLCQnhrjTObN++fdmyZSNHjqSfdnZ2/vDDDwcPHgwNDd26dauFhQW35ak55oKJ9CFpNTU19DgaM97t6OiIDAIAAAAASIaRNQBQF6zrNz148KCzs5M+XoY+h9TJyQmXG7t3796uXbsaGxv37Nnj5+fX0NDg6uoqFAqTkpKCgoK4rk6lNm7cGBsbO2HChOvXrwuFwoSEhD179kyePPmvf/0r7lcrjnXjkZqaGiMjI2YgG/kCAAAAAJANRtYAQH21tLQUFxczwwHV1dU6Ojqix9TweDxp2nn+/PmpU6fee+89IyMjZdesGr/88svnn39ubGxsbm4eHx/f0dFhZWV15coVX19frktTkc8///wf//hHY2OjpaXlp59++v333wcHB3/22WcaOab2/Pnzs2fPLlu2TPozMUUvdFhWVlZVVWVlZcUEx8PD46WXXlJqzQAAAAAAgwRG1gBgIBG/DhR93A19UJunp6ednZ34q3799depU6eamJjs2rVr5cqVGnOCW1JS0rvvvtvU1EQ/tba2TktL8/Dw4LYqFThw4MDu3buZFR85cuT169ft7e25rUoZhELhqVOntm/f3tzcnJOTw5z3ysK6YUhdXd0rr7zC4/HocbQxY8Zo5IAjAAAAAIA6wMgaAAxsrHPcWMfmeHl5WVtbx8bGfvTRRwKBwNTU1MzM7NixY7NmzeK6cAVYu3btP//5z87OTmaKjY3NzZs3R40axWFVyhYbG7t169aGhgZmyrBhw/7zn/+MHz+ew6qUISUl5cMPP6yrq2tubh46dOi//vWvefPmsW4Gwufzm5qaHB0dmfM6vby8hg4dynXtAAAAAACDBUbWAEDTVFVV8fn8e/fu8fn8+/fvt7W1WVhY3Lp1i5nBwsLC2dn5n//8p5eXF4d1yok+LunFixes6cOHD//1119HjBjBSVXKFhcXt379+sbGRtb0wMBA0Y94oMvOzl69enVZWRlzXB4h5M0333z69Gl3d/fIkSNHjx7t4eHh5uY2evRoQ0NDDksFAAAAABjkMLIGwLHq6urCwkKuq9BkFEV98MEHZWVlrOnGxsavvvrqhg0bzMzMOClMTp2dnTdv3iwrK3v48OGjR4/a2toIIdra2m1tbUZGRt9++625uTnXNSrYr7/+GhkZqaura2BgQFGUtra2hYXFyy+/7Ozs7Onp6e3tzXWBClBXVxcdHV1UVCQ6pkbz9vbeu3evjo4OJ4VpKi0trUmTJunp6XFdCAAAAAAMVBhZA+DYRx99VFFRMXz4cK4L0WT/+te/mGO7dHR06LGJIUOGmJmZjR8/3traWs72MzIy/P399fX15S1UPgKBICUlxc3NraGhwcXFZYCOGEpQWlra3t5ubm5uamo6dOhQjblenqiamprs7Oympia6dxYIBEKhkP7T0KFDFy9ezGl1GigzM/PYsWOTJ0/muhAAAAAAGKgwsgbAsXXr1oWFhQUHB3NdiMZ6/vy5ubn5sGHDeDyer6/vhAkT3N3d3dzcFHiUyuuvv3727FkbGxtFNSgzDw+P/Px8rqsABWhpacnPz8/Ly7t161Zubu7jx48FAkFzczPXdWkafAMDAAAAgJxwUgkAaDhDQ8O2tjacQwcDy9ChQydMmDBhwoQ1a9bQU1pbW7ktCQAAAAAAxGngqTQAACwYVgMNYGJiwnUJAAAAAADAhpE1AAAAAAAAAAAAWWBkDQBAuTo7O3k8Xmdnp2wvLykpUUgZJiYmCQkJDx48oJ/eu3fvq6++srCwiIyMpKcUFBRMnDjxvffea2hokH9xFy9eVEjjWVlZxcXFkue5d+9ecXGxopbYI6xOj9RhdSiK2rJlC4/H8/T0vH37Nququ3fvxsTEuLq6yrkUAAAAAIBeUQDAqbVr16alpXFdBcglODj46dOnEmZ49OiRbC1nZWW99tpr0s/v7u7e25/s7e2Zxzdv3kxKSqIoavHixfr6+leuXKGnHz58OCcnR7ZSxSmq8WPHjkl4A5l1UeASe4TV6RHnq3P+/Pnbt29TFLV3795x48aJV/XixQsXF5feXo5vYAAAAACQE45ZAwBQutra2o6OjvPnzwcGBp44cSIwMPDgwYOEkFOnTgUHB+/cudPX15e+UH10dLSfnx8hJCoqytXVNScnp7KyMi4uTigUOjs7V1VVyV+MUCjcv3//7NmzCSHOzs5RUVFLly6trKwkhJiZmZmamtKzRUVFnTt3bsOGDXFxceKV06Xu2bNn+vTpT58+7XFBvTUu2jIhRLxxVsuLFy/euHFjn+sizxI1bHX6XBcpG5fmjVLs6sjw0fj7+9ORWbJkyfPnz+mJEqoCAAAAAFAwrof2AAY7HDGhASQfs5aRkaGrq1tfX//o0SMzM7OmpqaysjJPT0+KooqKiqytrdva2lpbW62trVNTU+/du+fr60tR1MOHD11cXO7cuUM/pSjqzp07QqFQciXSHLP222+/LVy4kH78t7/9jaKo5cuX+/v7d3R0nD59+sGDBxRFJSQkfPDBBxRFdXR0DB8+PDU1lVV5fHz8xYsXKYp6//33Dxw40OMSe2yc1XJBQQHrbemxZVtb28bGRvFFiK6LzEvUsNWRZl2kbFzKz12BqyPblka7ePFiREQE85SpCsesAQAAAIBS4Zg1AADlCgoKGjZsGCFET0/P0NDQ1NR0+PDh1dXVhBB9fX1LS0sjIyNjY+OAgIA7d+5oaWnRr2IeMLy8vMQnyoDP51taWopO+eabb7q7uzdt2sRMuXr16tixY+maAwMDMzMzWZWnpKTcvXv3u+++mzx5sq+vr4TFsRpntZyamsp6W3ps2cXFhc/nS7MuMixRw1ZH+nXps3EpP3cFro7MW5pQKLx+/fr27duZKb1VBQAAAACgWDpcFwAAoPmkGRGzs7OzsrJSQTE6OjrNzc2iUwwMDBISEnx9fUtLS4OCguiJtbW19AMjIyMDAwNWIw0NDaGhofPnzyeEdHR0SFiceOMytGxiYiIUCqVZFxmWqGGrI/26aNLqnD59+pNPPtHX12em9FYVAAAAAIBi4Zg1AAClY44Tpp92d3czj5md//z8/DfffNPMzKy+vp4QUlFRIRAIDA0NW1tb6Rb4fD7zKnl4enoydwitqamhHzg4OMTHx6ekpNBPZ8yY8fPPP9OPCwoKpk6dyqrcx8cnJiamq6uro6MjOTlZKBSKXwOrx8ZZLYeEhLDeFlbL9J/q6+vd3NzElyK6LjIvUcNWR5p1kbLxPj93ha+ObFva5cuXg4KCHBwchELh/fv3RasSX3EAAAAAAMXCyBoAgHJlZGTU1tZeunTp2rVrDQ0NeXl5ycnJzc3NWVlZhJAnT57ExsbGxMRs3rzZ1tbW3t4+KCgoJCQkLy/P1NS0paXF2Ng4IiKiubl55syZCrmDgZeXl6WlJUVRiYmJiYmJP/74Iz19ypQpe/fupR/PmTMnNDR03759Z86c2b59e1FREavy9evX6+npOTs7v/vuu9OmTfvll18cHBzKysqYpfTWOKvlESNGsN6WgIAA0ZYJIS0tLa6ursOGDRNfCrMu8ixRw1aHtUTxpUjfeJ+fu8JXR4Yt7eLFi/SfnJyc7Ozs6DOvmar6jAMAAAAAgJy0FHIEBADIbN26dWFhYcHBwaITb9++nZ+f//7773NUFPTP66+/fvbsWRsbm/6+sKKiYvr06QUFBYqqxMPDIz8/v8c/jRgxghmYu3fvXlFR0YIFCxS1XELIhQsX3njjDTMzMwW2SYuJiZk2bRqPx+txKcpYlx4XpCiqXx3lrQtR19URraqjo8PT07OkpKTHOXv8BgYAAAAAkB6OWQNQR2VlZd9++6308/e209gvfTbS2dnJ4/E6OzvlXFBKSspLL70UGBh44MCBFStWbN68ubu7W5raLl++rKenFxYWtnv37rfffvv8+fPSvEoCRa2RzFpbW9va2lSzrGfPniUkJNAH+4wZM8bFxaWwsFBRjQsEAldXV2WM3eTm5oaGhtJDJD0uReHr0tuCFEL1q6O8dSHqujqiVeXn58fHxytq6QAAAAAA4nDMGgDHejxiIjs7+6OPPvrll1+kaSE7O3vTpk3p6enylCFlI48fP37ppZfkWRBtzpw548aNi4yMFAqFY8eO/eSTTyQcoCda28iRI3ft2hUeHl5aWurq6pqfn9/bpZRUuUYyH7MWHR1dU1Mzbdo05vJScpJwzBoAiMMxawAAAAAgJxyzBqCmBALBX/7yFycnp7/97W/0lOjo6D179kyfPv3p06cnT54MCQnZuXPnpEmTcnJyKisr4+LixBs5depUcHDwzp07fX1916xZI/oqVoNMI6LztLS0REZGJiUlvfPOOwKBgBBSW1vb0dFx/vz5wMDAEydOBAYGHjx4UEJ5EydOdHZ2Fr862JAhQ+gH2traFhYW7e3tUq4g80IHBwdtbe1Dhw6peI3oBSnKxo0b9+/fr6hhNQAAAAAAAFAxHLMGwLHejlmbO3duWVlZU1OTs7NzWlragwcPDAwM5s6du2LFCg8Pj1mzZvn7+5eWlra2tjY1Na1ater3338Xb7y4uDgoKKi8vJyiKCcnp507d27dupV+1W+//Sba4NSpU+lGioqKmJbLy8svXbp06NChBQsWfP75583NzW+88cbjx48FAoGbm1tlZeWzZ8/mzJlz7969s2fPSihv7NixWlpaooW99dZbnZ2dU6dOTU9P5/F4u3fvTkpKkmYF3dzctm/fPnPmzP3792dmZh49ejQwMFDFa+To6Mh6n2U+Zk3hcMwaQL/gmDUAAAAAkJMO1wUAQM9GjBihp6dnbW39xhtv/P7771lZWQ4ODo2NjZMnT3Z0dDQwMLCxsbGysrKyssrLy+utEX19fUtLSyMjI0JIQEBAYWEh86rdu3eLNsi8RLRlR0dHPz+/6OjonJycx48fT5s2jb7Xnp6enqGhoampqb6+fnV1NSEkJSWlt/J6q2306NHBwcEWFhYHDhyYPHmyhBZYK3jlypWnT5/6+/t//vnnjx8/Voc1EgqFjY2Nurq6Un64yiMUChsaGriuAmDAoI9dBQAAAACQGUbWANTd0KFDra2tGxoaQkND58+fTwjp6Oh48uRJf9uxs7MTHRViNdjjJcarqqqWLVt26NCh3377jZ7COvSst9akKW/o0KHe3t7e3t7FxcWffvqph4eHlC3MmjUrPDy8zxpUuUbNzc2rV6/W09OTMI9qPH36dOHChVxXATBg1NTULFmyhOsqAAAAAGAAw8gagLorKyubMmVKQUFBTEzM3Llzu7u7k5OTx40bx5zKbWho2NraSgihKEp8nEgoFNIP8vPzV6xYcfr0afqpj4+PaINjxoyhGxEKhUzLcXFxfn5+Pj4+9fX19O07KYoSPYW8u7ubfspqTbQ8Pp/v5ubGKkz0ZqDPnj2ztbWV0ILoCnZ2dnZ1dTGvFa1HZWskztzcXH3OBk1JSeG6CoABY926dVyXAAAAAAADG0bWANSRs7OznZ3dN998o6end+DAAXNz8/Xr1//vf/9zdnaeMGHCiRMnTp48WVlZeePGjeDgYBcXF2Nj44iIiG3btg0dOpTV1JMnT2JjY7u6ujZv3vzbb78xr2I1aGRkRDdiZGTEzDNp0qSVK1cOHTp05MiRx48fpyiqtrb20qVLxsbGDQ0NeXl5RUVFzc3NWVlZvZX32muvzZw5Mz09/ZVXXmFKunHjRlZWVm1trY6OTmlpaU1NzfHjx1966aU+V9DLy6uiouLHH38MCgpycHAghCQlJal4jXA9JgAAAAAAAGDgDgYAHFPq9bMrKiqmT59eUFCgjMaBgTsYAAxQuIMBAAAAAMgJx6wBaI5169Yx534SQuzt7efNm9fW1sZhSaABLl++fP369aioKMmz8fn8lJSUDRs2qKYqkE1LS8t3331nZWUVGxv7008/0bc3AQAAAAAAmWlzXQAAKMzRo0djRERERKSkpISHh1+7do3r0qBXJSUlCp+TJS4u7sCBA7K9lhDi4+Nz6tSpPmcTCoU7duzo8U/p6enjxo179OiRhJdLLvLevXvFxcUXL160sLCIjIykJxYUFEycOPG9995TyO1QKYrasmULj8fz9PS8ffu2NPP0+JJvv/32yJEjX375ZXp6uvhTQkh9ff2nn3566NAh+mlWVlZxcXFvVYm/LdK8mSydnZ08Hq+zs5MQRE0XHwAAIABJREFUsnv37oCAgEWLFh0+fFhHp+//XaO3OtEW+sTtRyn+oUh+hwEAAAAA5ISRNQBNtnHjxv3790+dOpXrQqBn2dnZq1atUuyc4iwsLIYNGybbawkhZmZm0szm6ura258mT5787NkzyRcfkFBkRkZGeXn5yJEj582bFxoa+uWXX/73v/8lhLi5uYWHh2/YsMHCwkKaCiVLSEhYuHBhWVnZ0qVL165dK8084i8pLi4+e/bsRx99tHnz5vfffz83N1f0KX0AaXd3d2dnZ3t7O92mr6/v9evXHz9+LOXbIs2byaKrq3vr1i1dXV1CSGZmprW1NSHE0dFR9BDXHjFbnWgLfeL2oxT/UCS/wwAAAAAAcsLIGgCAskRFRZ07d27Dhg1xcXGEkOjoaD8/P3o6PQ6Vk5NTWVkZFxd36tSp4ODgnTt3+vr6rlmzRsKcQqHQ2dm5qqpK+jLc3d09PT2Tk5PHjh0bGxs7b968devW/fTTTytWrAgNDSWEtLS0REZGJiUlvfPOOwKBoL29/dChQ2fOnGEdg7Z37941a9YUFhZGR0fv2bNn+vTpT58+JYQcOXLkzJkze/fulVADPSgjoQa6SPG1EwqF+/fvnz17Nv3U2dk5Kipq6dKllZWVhBAzMzNTU1Pxt/r8+fOBgYEnTpwIDAw8ePAg/eaL1izO39+ffs+XLFny/PlzaeYRf8mZM2ecnZ0JIUOGDLGyslq0aJHo059++okQYmtra2JiItrs4sWLN27c2OMS6beFEHLs2LEzZ86sWrWKGeGS8KmJf4K1tbUdHR03btwoKSn54Ycfvv7661GjRtFHs4q2zGqT2eqYFqR5qxX4UYo33udH2ePnKOEdBgAAAACQE0bWAACUIjExsaSkZOHChfv379+6dWthYSFz8ODbb79NP/D397eyslq6dOnEiRP5fP7mzZvT09MvXLgwZMiQ3ubU1tZOTEy0t7eXvhInJyd/f/9p06bl5+cvWLAgMTHx9OnTI0aMOHny5MOHD+vr67Ozs5ubm+fMmSMQCAoLC69evdrd3b1kyRJ/f3+mkcLCQmtr6+PHj+fl5Tk5OUVERAwfPjwuLi4tLS0/P3/JkiUrV67ssxIJNdBFiq9dVlaWoaGhaCNr166dNWtWWFiYQCDo7a0OCgri8/lhYWFnzpw5ffr02bNnRWvusTbm9rU5OTlvvfWWNPOIv6SwsJAeSiOEODs7FxcXiz4tLS3tsVlzc/P09PSmpibxP9FvS3JyckNDw5IlS+bMmfPbb7/Rf5LwqbE+wczMTD8/v9bW1uDgYDMzs0WLFn388cd+fn5aWlqsllltMlsd00KfbzVTs0I+SisrK1bjfX6UPX6OEt5hAAAAAAA54Q4GAABKcfXqVS8vL0KInp5eYGBgamrq5MmT6T9paWmxZtbX17e0tKQvJx8QEJCXl9fbnIQQutn+0tXV1dbWps+2s7a2pk8JtLOzKykpCQ4O9vPzi46OzsnJefz4sb+//9q1axsbG5nLYHV1dS1fvjw1NZUQkpKS4uDg0NjYOHnyZEdHxx9//HH8+PGEEGlujSqhBisrqx7Xjs/nW1pastr55ptvgoKCNm3aRB+dJP5Wh4eHGxoampqa6uvrV1dXs2qWUKFQKLx+/fr+/fuln0f0qZaWFnPxMnp4VPQpM2AqzsXFhc/nBwQE9PjXpKSkJUuWEELmzJlDCImJiSGESPjU6urqRD/BoKAg8dMz6U2L1TIhRLRNOzs7eiLTQp9vtegi5P8oMzMzWY1L+VGKf46S32EAAAAAAJnhmDUAAGWpra2lHxgZGRkYGEj5Kjs7O2aYSUmYATstLS2hUFhVVTVz5szJkyfTY2R2dnaZmZnXr19fuHAhPZuOjs7UqVO3bNlCCGloaHB3d1++fPny5csnTJjwxx9/dHd3y19Db7Pp6Og0NzezJhoYGCQkJJw9ezY+Pp6eIvmtZtUsoarTp09/8skn+vr60s8j+tTV1ZU5MK20tNTDw0P06ahRo3pr08TERMKb8McffzCX9meusCbhUxP/BHscohVvmdWmKKYF2bZqotqPUvxzlPwOAwAAAADIDCNrAABKMWPGjJ9//pl+XFBQEBISYmZmVl9fTwipqKigT38zNDRsbW0lhFAUxez25+fnh4WFSZiTz+f36wL24pjF0e3ExcX5+fn5+PjU19d3d3fHx8fb2tqmp6eXlZVRFEXPs3379tTU1KtXr/r4+MTExHR1dXV0dCQnJ3t7e1+8eJEQ0t7ezpzTJ0MNDNbaeXp6PnjwgHlaU1NDP3BwcIiPj09JSSE9vdVMC93d3RRFsWoWCoU9XqLr8uXLQUFBDg4OQqHw7t27fc5z//591tOlS5fSt6EUCoVtbW3fffed6NOQkBBm3VnN1tfXjxo1qrdrwI0fPz4qKurFixdtbW3Jycn0RAmf2g8//CD6CdJLpB8IhULmgVAoZLXMalN0q6P1+VYr9qNkTqBmGpfmo2R9KMw77Obm1uPbCwAAAAAgD4ysAQAoxZw5c0JDQ/ft23fmzJnt27ePGDHC3t4+KCgoJCQkLy/P1NQ0NzfXxcXF2Ng4IiKitbX1yZMnsbGxMTExmzdvHjduXG9zNjc3z5w5s193MKAlJyd3dXXdunUrLy/v0aNHFy9efPjw4YMHD27evBkUFJSUlLRz586RI0ceP368oaFhy5Yt165d+/jjj7W0tC5dutTa2nr//v1NmzYtWrTI0dFRT0/P2dn53XffnTZt2ocffqilpfX2228fPXqUx+NdvXpVfNG3b9+urq6+fPmyhBqY4R7W2nl5eVlaWtJ/TUxMTExM/PHHH+k/TZkyhb5tgvhbfe3atYaGhry8vOTk5Obm5oCAANGaf/nlFwcHh7KyMtEiL168SP/VycnJzs6uqKioz3lu3rwp+nTYsGGjRo2aN2/eyZMnv/zyywMHDowbN070KX3jgpKSkhs3bty+fbu8vJxutqWlxdXVtccl0tatW2djY+Pq6rp69WozMzP6zZw0aVJvn5pQKBT9BDMyMmpray9dunT//v3q6uqkpKSKigo+n//zzz9/8MEHTMshISGsNm1sbOit7sqVK3QLfb7VWVlZdM0K+SiLiopYja9fv17yR8n6jOiTWOl3WJ475AIAAAAA9EZLzgMfAEBO69atCwsLCw4O5roQkN3rr79+9uxZaS401puKiorp06cXFBTIWYmHh0d+fr6cjaibe/fuFRUVLViwQIFtXrhw4Y033jAzM5NzHoWIiYmZNm0aj8dT2RK5wtVHybzD4n/CNzAAAAAAyAl3MAAA4F5ra2tbWxvXVSjGunXrRC9oZW9vHxERIU+DY8aM6e7uLiwsHD16tNzVEUKIQCBwdXWVPBYjzTwKkZubGxoa6ujoqLIlcoiTj5J5hxWyRAAAAAAAFoysAQBwLyUlJTw8/Nq1a8yFpQauo0ePKrxNb29vBbamp6fn6ekp/zwK4ePjo+Ilckv1HyXzDgMAAAAAKANG1gAAuLdx40auSwAAAAAAAIB+wx0MAAAAAAAAAAAAZIGRNQAAAAAAAAAAAFngbFAAjtna2i5fvlxHB2EcwJqbm8ePH6+tzf3/VTQ1NTk7O6tyid3d3dra2lpaWqpcKICiPH/+fPXq1VxXAQAAAAADmBZFUVzXAAAAA9W6devCwsKCg4O5LgQAAAAAAIAD3B9hAQAAAAAAAAAAMBBhZA0AAAAAAAAAAEAWGFkDAAAAAAAAAACQBUbWAAAAAAAAAAAAZIGRNQAAAAAAAAAAAFlgZA0AAAAAAAAAAEAWGFkDAAAAAAAAAACQBUbWAAAAAAAAAAAAZIGRNQAAAAAAAAAAAFlgZA0AAAAAAAAAAEAWGFkDAAAAAAAAAACQBUbWAAAAAAAAAAAAZIGRNQAAAAAAAAAAAFlgZA0AAAAAAAAAAEAWGFkDAAAAAAAAAACQBUbWAAAAAAAAAAAAZIGRNQAAAAAAAAAAAFlgZA0AAAAAAAAAAEAWGFkDAAAAAAAAAACQBUbWAAAAAAAAAAAAZIGRNQAAAAAAAAAAAFloURTFdQ0AADDA/Oc///nyyy8JITU1Nebm5sbGxoaGht9///2wYcO4Lg0AAAAAAEB1dLguAAAABh59ff3c3NyWlhZmipmZmampKYclAQAAAAAAqB7OBgUAgH57/fXXDQ0NRafMnj1bRwf/WwMAAAAAAIMLRtYAAKDftLW1Z82apaWlRT+1tLT88MMPuS0JAAAAAABA9TCyBgAAsvh//+//MVdV09XVDQgI4LYeAAAAAAAA1cPIGgAAyGL8+PH6+vqEEC0trbCwMOb4NQAAAAAAgMEDI2sAACCjRYsWDRkyxNLScvXq1VzXAgAAAAAAwAEtiqK4rgEAAAYkPp8fGBhoZmZWUVHBdS0AAAAAAAAcwH3cADjz8OHD0tJSrqsAkIu+vn5QUNC1a9e4LgRALgEBAcbGxlxXAQAAAAADD45ZA+BMeHg4RVEWFhZcFwIguydPnpibmxsYGHBdyABw7dq1qVOncl0FaW5uLiws9Pf357oQNZKdnb1169awsDCuCwEAAACAgQfHrAFwhqKoyMhIDw8PrgsBAFXw8PA4fvw411WQ/Pz8L774Qh0qUR87duzAfzQCAAAAgGxwBwMAAAAAAAAAAABZYGQNAAAAAAAAAABAFhhZAwAAAAAAAAAAkAVG1gAAALjX2dnJ4/E6Oztle3lJSYlCyrh3715xcfHFixctLCwiIyPpiQUFBRMnTnzvvfcaGhrkbJ+iqC1btvB4PE9Pz9u3b0s5j/iUb7/99siRI19++WV6ejr9KvEp9fX1n3766aFDhwghWVlZxcXFchYPAAAAACAOI2sAAADc09XVvXXrlq6urgyvzc7OXrVqlfw1ZGRklJeXjxw5ct68eaGhoV9++eV///tfQoibm1t4ePiGDRvkv5dxQkLCwoULy8rKli5d+v+1d+9BTV15HMAPD8M7oEChFAoaYhVQoAaqMS6MSnkWqQhsV9wdkaldbYV1sd1dZAGLq1ueM91tdBXECrOIGlRwaolaWbSOLgWpJkARWhAEIpaHAZdHcvePO5vJJDwir6D7/fx1czj3d36/+8t0psebe3ft2qXhHJWRH3744fTp0x9++GFCQsL27dsHBgbURwghMplsZGRkcHCQEMLhcK5evdrR0THN/AEAAAAAVGBnDQAAYF6QSCRDQ0Nnzpzhcrm5ublcLjczM5MQcuLECV9f3wMHDnA4nJ07d+bk5Hh5eRFCsrKy2Gw2IaS6urq1tbWgoEAul7NYrIcPH05hdblcnp6e/s4779AfWSxWVlZWdHR0a2srIcTc3JzJZNKLFhcXx8XFFRQUqKdKCMnJyTl48GBAQEBXV5f6Kt7e3nTyW7duffbs2ZiZqM9RGSksLGSxWIQQPT09Kyur0tJS9RFCiI2NjampqSLse++9Fx8fP4UrAwAAAAAwAeysAQAAaN+NGze8vLykUimPxxOLxREREYWFhfn5+YSQtWvXisXihISEioqKkpISPT09+pTNmzfTB97e3lZWVtHR0bq6ugKBwN7efgoJVFVVGRkZKY/s2rUrJCQkIiJieHiYHhEIBI2NjZGRkenp6Z988omVlZVKqqdPn168eHFiYqKdnV1BQYH6Kq+//jp9UF1d/e67746ZifoclZH6+np6H40QwmKxmpqa1EfUw1pYWFRUVPT19Wl8SQAAAAAAJoedNQAAAO3j8XiLFi0ihDAYDCMjIyaTaWdn19bWRggxMDCwtLQ0NjY2MTFZs2ZNbW0tfYqOjo56HHd39zHHJyUWiy0tLVUGjxw5IpPJ9u7dS38sLy9fuXIlnSSXy71x44ZKqkKh8Pvvvz958qSPjw+HwxlvLblcfvXq1aSkpAnyUZ+jGNHR0dHX16cH9fT09PT01EfGjOns7CwWiye+DgAAAAAAz0Vf2wkAAAAAIePslKmwtbWd/sPOxqSvr9/f368yaGhoeO7cOQ6H09TUxOPxCCESiYT+k7GxsaGhocr8np6ewMDA8PBwQsjQ0NB4a+Xn5//+9783MDCYIB/1OYoRNputuCutqakpPDx8YGBAZWTMmKampnK5fIJFAQAAAACeF+5ZAwAAmBeo/6E/ymQyxbFiP0gkEkVERHR3dxNCWlpa6N9pGhkZSaVSOoJYLFac9Vzc3NwePHig+Nje3k4fODo6FhUVCYVCQkhQUNDXX39Nj9fV1W3cuFElVU9PTz6fPzo6OjQ0VFZWJpfL1Z+2dunSJR6P5+joKJfL79+/r8kclREPDw/6RZ9yuXxgYMDPzy86OlplRHFJlcN2d3cvX758ChcHAAAAAGA8uGcNAABA+yorKyUSyYULF0xMTHp6empraxsaGvr7+6uqqqytrTs7O48dOzY6OpqQkLBq1Soej+fn5xcaGspkMmtqatzd3U1MTBITEz/++OPg4OCKigrFg8k05+7ubmlpSVGUjo6OQCAQCARBQUFbtmwhhGzYsOHQoUOEkNDQ0Nra2sOHDzs4OCQlJTU0NKik+tFHH3377bcsFmv16tW5ubm3bt3asGGDWCxesmQJvcr58+d37NhBvwxhYGDg7t27msxRH+no6MjLy+vq6srIyDA1NX3jjTfCwsKURwghjY2N169ft7S0/Omnn5ycnJ4+fcpms+mf3AIAAAAAzBSdqf3LNgBMX2RkZHJysqurq/LgnTt3RCLR9u3btZXVi0UsFguFwri4OG0nArPuJei1q6urSCSawoktLS0BAQF1dXUzkoZIJEpNTS0uLlb/07179xoaGujdtJlSUlKyfv16c3Pzac6ZPj6f7+/vr9i/U5aSkuLi4hIZGTmrCQAAAADASwm/BgWYX5qbm//xj39oPr+xsXH6i04aZGRkZMmSJSMjI5OGqqioWLVq1aNHj6aflSbkcnlKSsr046hcAe3WKxQKX331VS6Xm5GRERMTk5CQIJPJJj6Fzv/SpUsMBiMiIiItLW3z5s1nzpzR5KwJTHAd6DdUhoaGtrS0TBxkYmKx+NNPP9VkJt3ry5cvL1q0SOWdkp999pmOjk5aWtqYF0q9zPHqmqmiZpxUKh0YGJiDhVasWOHs7FxfXz9TAYeHh9ls9sRbZprMmb6amprAwMAxt9UAAAAAAKYDO2sA8wubzdZ88nfffRcbGzvNFTUJsmDBgps3by5YsGDSaD4+Pk+ePJmzm2Gf63KNR/0KaLdePz8/Ly8vf3//hISE48ePX758+csvv5xgviL/4OBgJyenLVu27N+/Pz09PSoqaoK7nKbZ93fffdfR0fGXv/ylo6OjZmWNLT8/n8/na7KJSfc6ICDAz8+vtLRU8bh6uVx+8+ZNQsiePXvU3wg5Zpnj1TVTRc04oVAYFRV15cqVOVjLw8Nj2bJlMxWNwWC4ublNf870eXp6Ojk5zfYqAAAAAPB/CDtrAPPO8PDw7373u8WLF+/fv58eycnJOXjwYEBAQFdXV15enp+f34EDB9atW1ddXd3a2lpQUKAe5MSJE76+vgcOHOBwODt37lQ+SyWgIojynKdPnyYnJ1+8ePFXv/oV/Yh0iUQyNDR05swZLpebm5vL5XIzMzPHzF+xYZGVlVVcXBwXF0dnODg4mJ2dXVhYmJKSonysHkFl9TEX/dvf/lZYWEg/+0ldWVnZypUrjx07FhYWtnv37tLS0piYmMDAQMWESa8AXS89+YsvvigsLIyNjaV3cMasVyXnzMxMExOT5uZmiqK2bdtWXV09ZhPXrl3LYrEePnyoElOxQ6Srq7tw4cLBwUENvwaKEx0dHXV1dbOzs2ev73p6euo7WcpNV+myetOlUqmFhYW1tfXZs2fpEU16vXTpUj8/v88//5z+ePHixU2bNinXPmmZirrUm6telPI1OXTokCZtpZeYQfHx8enp6YrXBQAAAAAAwDxCAYCWRERE3L9/X2WwqqrqtddeGxoakkgkZmZmVVVVRUVF58+fpyhq+/btGRkZ9fX1TCbz8ePHP/744927dzkczpjBGxoarK2tBwYGpFKptbU1n89XnKUSUBFEOfI333wTHx9PUVR4eHhtbW1lZeWCBQu6u7sfPXpkbm7e19fX3Nzs5uY25tLOzs5tbW3nzp374IMPKIoaGhqys7Orq6srKSlJT0+nKOrSpUvKx+oRVFZXX/TatWt08NbWVgsLC/UIw8PDurq6P//8s0wmMzY2rqmpoSjqjTfeePz4MUVRk16BU6dO0fVSFFVaWpqWlkZR1IULFzIzM8erVyXn0dFRe3t7+i2NqampEzdRLperxAwLCwsODs7Ozg4LC9u7d+/g4KCGX4Nly5YVFhb29vYmJib6+Pjcv39/9vru7OxcVFSknLZK0w8dOqTcZfWmHz16tKmpic/nr169mh7RpNf79++/fPmymZlZX18fRVF79+6lf+8plUo1ae6PP/6oqEu9uepFKV+T6upqDduq/j2hKMrFxWXM8Tl2//79iIgIbWcxvyQnJ58+fVrbWQAAAADACwnvBgWYdxwcHBgMhrW19fr16//9739XVVU5Ojr29vb6+Pg4OTkZGhq+8sorVlZWVlZWtbW14wUxMDCwtLQ0NjYmhKxZs6a+vl5xVlpamnJAxSnKkZ2cnLy8vHJycqqrqzs6Ovz9/ekX6jEYDCMjIyaTaWBg0NbWNkEV5eXl7u7u9ClcLvfatWthYWG7du3q7e1NTk5+/Pix4lj9XF9fX+XVX3vtNZVFBQLBW2+9RQh55ZVXxlx9wYIF9N1ehBBra2tra2tCiK2tbWNjo5WVlVAonPgKREdHJyQk0IMXL17cunUrISQ0NHSCelVyXrlyZUxMTG5u7v79+21sbFRWVL7U4wVctmyZr6/vwoULMzIyfHx8Joig8jX46quvurq6vL29//znP3d0dMxl31WaLpVK8/LyFF329vZWaXpTU9P777+/devWffv23blzx9vbW32hMXvt7+/v4OBw/PjxwMBANzc3HR0dxZ8mbS5dJl2XJs1V7qxEIplOW+VyeU9Pz3gLzZn+/v6RkZH5kMn88Z///EfbKQAAAADAiwo7awDzl5mZmbW1dU9PT2BgYHh4OCFkaGios7PzeePY2toq/6++SsAxn1b+8OHDbdu2ZWdn3759mx5R3rzQkEQioQ+MjY0NDQ1tbW1v3LixdetWkUh07tw5xXFJScmkq6ugb0bTMA1F5jo6OnK5nGh2BRRn/fzzz4o9CIqixrsO6jnHxMSsXr3a3d09JCSkvLz8eZtoZmbm4eHh4eHxww8/fPzxx66urhpGCAkJiYqKUh+f7b7Tz0pTbrqTk5Nyl5W/ACUlJTdu3Ojv7z948CAhZNWqVZ9//vmpU6fUw47X6/j4+L/85S9PnjxJSkpqb29/rjIVdU3a3JGRkc7OTuVrMp229vb2zoe3Tw4MDDx69Gg+ZDJ/tLe3v/nmm9rOAgAAAABeSNhZA5i/mpubN2zYUFdXx+fzN23aJJPJysrKVq1aRf3veflGRkZSqZSMsylA7yIRQkQiUUxMTH5+Pv3R09NTOeCKFSvoIPRvEuk5BQUFXl5enp6e3d3d9L4GfZurIrhMJqMmfGx/UFDQ4cOH6buT6urq0tLSioqKNm3aVFFR4eXl9c9//jMsLIw+Vk9eZXXFQopjDw+P8+fP79ixY3BwkH4c2AQoiqIvhSKOJldAUe9bb72VlZUVEBAgk8muXbv2zjvvjLmK+hVzdHSk89y2bZvKispNFIvFy5cvV7kCyntJT548sbGxmSCC8tdgZGRkdHRUufbnqvq5+q6cJEVRiYmJKk13cXGxsbFRdFn5C0BR1MmTJ/l8vr6+PiEkICCAy+V+9tlniof0TdBreh8tOjr6T3/6k66urqGhIT1O56NJmYq61JurXtTChQuVr4mGbR3TokWLhELhBBPmhkgkSk1NLS4u1nYi88iMvGIYAAAAAP4/YWcNYH5hsVi2trZHjhxhMBgZGRkWFhYfffTRt99+y2KxVq9enZubm5eX19raev36dV9fX2dnZxMTk8TExD/84Q9mZmYqoTo7O48dOzY6OpqQkHD79m3FWSoBjY2N6SDGxsaKOevWrduxY4eZmdnSpUuPHj1KUZREIrlw4YKJiUlPT09tbW1DQ0N/f39VVRWHw1Fe9M6dO21tbZcuXXr//fdra2sPHz7s4OCQlJTk4OAgk8n27dsXEhKyZ88euVyuOFbfE1RZva2tTWXR3/72t5WVlZs3b37zzTeXLFlSXl7+9ttvK0coKysbHR29efOmqanpo0ePzp8/HxIS8uDBg3/9619cLnfSK6Cnp0fXu2PHjt27d9++fZvNZq9bty4vL08lVUW9KjnzeDwmk/mb3/yGfjvBeE38xS9+ERwcXFFR8frrrytiXr9+vaqqSiKR6OvrNzU1tbe3Hz169NVXX530a+Du7t7S0nL27Fkej0e/3fLixYuz1PeOjo6Wlpb8/PzW1tYnT55UVFSsW7cuNDRUuem9vb3KXVb+Anz55Ze3bt1qa2ujf7Apl8vlcvnu3bv9/f0n7vUHH3wgEAiCgoK2bNny4YcfxsbGSqXS48ePE0K++OKLffv2aVJmZWUlXZdKc0tLS9WLUu/spG319fWd8n8BAAAAAADgxaIz8T+wA8DsiYyMTE5OdnV1nY3gLS0tAQEBdXV1sxEcNFRQUMDj8ZSf9gUvgSm31dXVVSQSzUJGzwf3rKlLSUlxcXHBL2QBAAAAYApwzxrAy2D37t2K334SQuzt7cPCwgYGBrSydGJi4twHmZE0ZnAhiUQyPDz8+PFjbKu9TNBWAAAAAABQgZ01gJfB3//+d5WRnJycqKioK1eubNy4cY6X1kqQGUljBhfKzc3l8/lfffXVbOcDc0lbbW1sbGSz2bMxecbdu3fPwMBALBZv3759z549qamphJC6urrY2Fg2m52dnU2/sfd5VVVVMZnMpUuXznS+AAAAAADTpauWtDb6AAAFEElEQVTtBABgVsTHx6enp8/2thqM549//GNra+ss/dQXtEUrbf3uu+9iY2NnY/KMq6ys/Omnn5YuXRoWFhYYGPjXv/718uXLhJDly5dHRUXFxcVNbVuNEMLhcK5evdrR0TGj+QIAAAAAzADsrAEAAGhTVlZWcXFxXFxcQUEBISQnJ8fLy4seZ7PZ1dXVra2tBQUFJ06c8PX1PXDgAIfD2blzp/pMQohislwuZ7FY9JsW5oZcLk9PT1e8PJfFYmVlZUVHR7e2thJCzM3NmUymerFnzpzhcrm5ublcLjczM5Mu6uDBgwEBAV1dXcrx33vvvfj4+DkrBwAAAABAQ9hZAwAA0BqBQNDY2BgZGZmenv7JJ5/U19crbjXdvHkzIcTb29vKyio6Onrt2rVisTghIaGioqKkpOSbb75Rmak8WVdXVyAQ2Nvbz1khVVVVRkZGyiO7du0KCQmJiIgYHh6mR9SL5fF4YrE4IiKisLAwPz//9OnTixcvTkxMtLOzo7feFCwsLCoqKvr6+uasIgAAAAAATWBnDQAAQGvKy8tXrlxJCGEwGFwu99q1azo6OvSfFAc0AwMDS0tLY2NjExOTNWvW3L17d7yZNHd39zHHZ4lYLLa0tFQZPHLkiEwm27t3L/1RvVgGg2FkZMRkMu3s7Nra2oRC4ffff3/y5EkfHx8Oh6MSzdnZWSwWz0EtAAAAAACawxsMAAAAtEkikdAHxsbGhoaGmpxia2trZWU1m0k9N319/f7+fpVBQ0PDc+fOcTicpqYmHo9HJiu2p6cnMDAwPDycEDI0NKTyV1NTU+U38wIAAAAAzAe4Zw0AAEBrgoKCvv76a/q4rq7Oz8/P3Ny8u7ubENLS0jI8PGxkZCSVSgkhFEUp9pVEItHbb7+tMpMQojxZLBZTFDVnhbi5uT148EDxsb29nT5wdHQsKioSCoVjFqvIUCaTURTl6enJ5/NHR0eHhobKysrkcrny09a6u7uXL18+R/UAAAAAAGgGO2sAAABaExoaGhgYePjw4cLCwqSkJAcHB3t7ex6P5+fnV1tby2Qynz59amJikpiYKJVKOzs7jx07xufzExISbGxsVGbW1NQ4OzvTk/v7+4ODg+fyDQbu7u6Wlpb0TplAIBAIBGfPnqX/tGHDhkOHDo1Z7JUrV3p6empra8vKyvr7+9esWcNgMFgs1q9//Wt/f/9bt245Ojo2NzcTQp4+fcpmsxctWjRnFQEAAAAAaEJnLv9BGwCURUZGJicnu7q6ajsRAJgLrq6uIpFoyqe3tLQEBATU1dVNMw2RSJSamlpcXDzNOOru3bvX0NCwZcuWGYxZUlKyfv16c3NzPp/v7++/ZMmSGQyukJKS4uLiEhkZORvBAQAAAODlhnvWAAAAXgBSqXRgYEDbWUxkxYoVzs7O9fX1MxVweHiYzWabm5vX1NQEBgbO0rYaAAAAAMB04A0GAAAALwChUBgVFXXlypWNGzdqO5dxeXh4zGA0BoPh5uZGCPH09JzBsAAAAAAAMwg7awAAAC+A+Ph4bacAAAAAAACq8GtQAAAAAAAAAACAqcDOGgAAAAAAAAAAwFTg16AAWmNraxscHKynp6ftRABgLvT19bFYLG1nQWQy2bNnz+ZDJvPHs2fPTp06pe0sAAAAAOCFpENRlLZzAAAAAAAAAAAAePHg16AAAAAAAAAAAABTgZ01AAAAAAAAAACAqcDOGgAAAAAAAAAAwFT8F3CnIl/j0hDyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(keras_model, show_shapes=True, dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up epochs and steps\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "eval_batch_size = 128\n",
    "\n",
    "train_data_size = len(tc_train_labels)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-30647f5546e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m tf_text.mask_language_model(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_selector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_values_chooser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as tf_text\n",
    "tf_text.mask_language_model(\n",
    "    input_ids, item_selector, mask_values_chooser, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python369jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}