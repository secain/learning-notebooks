{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -U tqdm\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix\n",
    "tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar\n",
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...等于前面所有维度都取:\n",
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicting Types\n",
    "Tensorflow不会自动执行任何类型转换, 需使用tf.cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]\n",
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "\n",
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40., dtype=tf.float64)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "基本不需要使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 1., 42.,  3.],\n",
       "       [ 4.,  5.,  6.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 1., 42.,  0.],\n",
       "       [ 4.,  5.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  4.,   5., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n",
    "                    updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[4., 5., 6.],\n",
       "       [1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回一个损失的张量，而不是返回实例的平均损失\n",
    "# 这样可以根据要求使用类别权重或样本权重\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6235 - mae: 0.9953 - val_loss: 0.2862 - val_mae: 0.5866\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 961us/step - loss: 0.2197 - mae: 0.5177 - val_loss: 0.2382 - val_mae: 0.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b08a506f60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading Models with Custom Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2054 - mean_absolute_error: 0.4982 - val_loss: 0.2209 - val_mean_absolute_error: 0.5050\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1999 - mean_absolute_error: 0.4900 - val_loss: 0.2127 - val_mean_absolute_error: 0.4986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b08b753ef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_loss.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn})\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当损失函数本身含有可调参数，如阈值时, load_model时也需要设置其参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  1/363 [..............................] - ETA: 0s - loss: 0.3597 - mae: 0.5905WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2226 - mae: 0.4892 - val_loss: 0.2540 - val_mae: 0.4907\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.2184 - mae: 0.4844 - val_loss: 0.2372 - val_mae: 0.4879\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "                                custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 或者通过继承`keras.losses.Loss`类，实现其`get_config()`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2152 - mae: 0.4801 - val_loss: 0.2327 - val_mae: 0.4751\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2122 - mae: 0.4763 - val_loss: 0.2164 - val_mae: 0.4728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", # TODO: check PR #25956\n",
    "                               custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 等同于`keras.activations.softplus()`或`tf.nn.softplus()`\n",
    "* 等同于`keras.initializers.glorot_normal()`\n",
    "* 等同于`keras.regularizers.l1(0.01)`\n",
    "* 等同于`keras.contraints.nonneg()`或`tf.nn.relu()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.5542 - mae: 0.8962 - val_loss: 1.4154 - val_mae: 0.5607\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.5943 - mae: 0.5256 - val_loss: 1.4399 - val_mae: 0.5137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b08db6ff60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 同损失函数一样, 当需要保存参数时需继承对应的父类\n",
    "* Note that you must implement the `call()` method for losses, layers (including activa‐tion functions), and models, or the `__call__()` method for regularizers, initializers, and constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: if you use the same function as the loss and a metric, you may be surprised to see different results. This is generally just due to floating point precision errors: even though the mathematical equations are equivalent, the operations are not run in the same order, which can lead to small differences. Moreover, when using sample weights, there's more than just precision errors:\n",
    "* the **loss** since the start of the epoch is the mean of all batch losses seen so far. Each batch loss is **the sum of the weighted instance losses divided by the _batch size_** (not the sum of weights, so the batch loss is _not_ the weighted mean of the losses).\n",
    "* the **metric** since the start of the epoch is equal to **the sum of weighted instance losses divided by sum of all weights** seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing.\n",
    "\n",
    "If you do the math, you will find that loss = metric * mean of sample weights (plus some floating point precision error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.7864 - huber_fn: 0.7864\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 766us/step - loss: 0.2497 - huber_fn: 0.2497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b087116e10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1156 - huber_fn: 0.2338\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1125 - huber_fn: 0.2282\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11558227241039276, 0.11599889315680392)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"huber_fn\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming metrics\n",
    "* 通常情况：For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch.\n",
    "* 当metric值不等于每个batch的metric的均值时(如分类任务中的precision), 需要一个流式指标对象, 每个batch后更新真实的metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "# batch 1\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch 2\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重置变量\n",
    "precision.reset_states()\n",
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.Huber(2.0), optimizer=\"nadam\", weighted_metrics=[HuberMetric(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3571 - HuberMetric: 0.7053\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.1226 - HuberMetric: 0.2420\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),\n",
    "                    epochs=2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35711368918418884, 0.3571138122580369)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"HuberMetric\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer with no weights\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "exponential_layer([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 构造函数(`__init__()`)接受基本参数(如input_shape, trainable和name), 保存超参数.\n",
    "* The `build()` method’s role is to create the layer’s variables by calling the `add_weight()` method for each weight. \n",
    "* The `call()` method performs the desired operations.\n",
    "* The `compute_output_shape()` method simply returns the shape of this layer’s outputs.\n",
    "* The `get_config()` method is just like in the previous custom classes. Note that saving the activation function’s full configuration by calling `keras.activa tions.serialize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)# a custom stateful layer (i.e., a layer with weights)\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.2563 - val_loss: 0.9472\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.6485 - val_loss: 0.6219\n",
      "162/162 [==============================] - 0s 549us/step - loss: 0.5474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5473727583885193"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_layer.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_layer.h5\",\n",
    "                                custom_objects={\"MyDense\": MyDense})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return X1 + X2, X1 * X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = keras.layers.Input(shape=[2])\n",
    "inputs2 = keras.layers.Input(shape=[2])\n",
    "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your layer needs to have **a different behavior during training and during testing** (e.g., if it uses Dropout or BatchNormalization layers), then you must add a training argument to the `call()` method and use this argument to decide what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.GaussianNoise\n",
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 9.1326\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.0577\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8868\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5831\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6460\n",
      "162/162 [==============================] - 0s 735us/step - loss: 0.6510\n"
     ]
    }
   ],
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: my_custom_model.ckpt\\assets\n",
      "Epoch 1/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8027\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5478\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4490\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5464\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6504\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_custom_model.ckpt\")\n",
    "model = keras.models.load_model(\"my_custom_model.ckpt\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model using the sequential API instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = ResidualBlock(2, 30)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The constructor creates the DNN with five dense hidden layers and one dense output layer.\n",
    "* The `build()` method creates an extra dense layer which will be used to recon‐struct the inputs of the model. It must be created here because its number of units must be equal to the number of inputs, and this number is unknown before the `build()` method is called.\n",
    "* The call() method processes the inputs through all five hidden layers, then passes the result through the reconstruction layer, which produces the recon‐struction.\n",
    "* Then the call() method computes the reconstruction loss (the mean squared difference between the reconstruction and the inputs), and adds it to the model’s list of losses using the add_loss() method.11 Notice that we scale down the reconstruction loss by multiplying it by 0.05 (this is a hyperparameter you can tune). This ensures that the reconstruction loss does not dominate the main loss.\n",
    "* Finally, the `call()` method passes the output of the hidden layers to the output layer and returns its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:230 __call__\n        reg_loss = math_ops.add_n(regularization_losses)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3512 add_n\n        return gen_math_ops.add_n(inputs, name=name)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:432 add_n\n        \"AddN\", inputs=inputs, name=name)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:589 _create_op_internal\n        inp = self.capture(inp)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:639 capture\n        % (tensor, tensor.graph, self))\n\n    InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=2957399270848); accessed from: FuncGraph(name=train_function, id=2957399933336).\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-02c0c12e003a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReconstructingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"nadam\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:230 __call__\n        reg_loss = math_ops.add_n(regularization_losses)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3512 add_n\n        return gen_math_ops.add_n(inputs, name=name)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:432 add_n\n        \"AddN\", inputs=inputs, name=name)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:589 _create_op_internal\n        inp = self.capture(inp)\n    C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:639 capture\n        % (tensor, tensor.graph, self))\n\n    InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=2957399270848); accessed from: FuncGraph(name=train_function, id=2957399933336).\n    \n"
     ]
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients with Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you may want to stop gradients from backpropagating through some part of your neural network. \n",
    "\n",
    "To do this, you must use the `tf.stop_gradient()` function returns its inputs during the forward pass (like `tf.identity()`), but it does not let gradients through during backpropagation (it acts like a constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 - loss: 0.0900 - mean_square: 858.5000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3500/10000 [=>....]'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A fancier version with a progress bar:\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "progress_bar(3500, 10000, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - loss: 0.0900 - mean_square: 858.5000\n"
     ]
    }
   ],
   "source": [
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "11610/11610 [==============================] - mean: 1.3955 - mean_absolute_error: 0.5722\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - mean: 0.6774 - mean_absolute_error: 0.5280\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - mean: 0.6351 - mean_absolute_error: 0.5177\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - mean: 0.6384 - mean_absolute_error: 0.5181\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - mean: 0.6440 - mean_absolute_error: 0.5222\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run this cell, please install tqdm, ipywidgets and restart Jupyter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of All epochs:   0%|<bar/>| 0/5 [00:00<?, ?it/s]>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tqdm\\std.py\", line 1090, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Anaconda\\envs\\studio\\lib\\site-packages\\tqdm\\notebook.py\", line 255, in close\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'sp'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tqdm.notebook import trange\n",
    "    from collections import OrderedDict\n",
    "    with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "        for epoch in epochs:\n",
    "            with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n",
    "                for step in steps:\n",
    "                    X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        y_pred = model(X_batch)\n",
    "                        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                        loss = tf.add_n([main_loss] + model.losses)\n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    for variable in model.variables:\n",
    "                        if variable.constraint is not None:\n",
    "                            variable.assign(variable.constraint(variable))                    \n",
    "                    status = OrderedDict()\n",
    "                    mean_loss(loss)\n",
    "                    status[\"loss\"] = mean_loss.result().numpy()\n",
    "                    for metric in metrics:\n",
    "                        metric(y_batch, y_pred)\n",
    "                        status[metric.name] = metric.result().numpy()\n",
    "                    steps.set_postfix(status)\n",
    "            for metric in [mean_loss] + metrics:\n",
    "                metric.reset_states()\n",
    "except ImportError as ex:\n",
    "    print(\"To run this cell, please install tqdm, ipywidgets and restart Jupyter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, converting a Python function that performs TensorFlow operations into a TF Function is trivial: decorate it with `@tf.function` or let Keras take care of it for you. However, there are a few rules to respect:\n",
    "\n",
    "* If you call any external library, including NumPy or even the standard library, this call will run only during tracing; it will not be part of the graph. Indeed, a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, and so on). So, make sure you use `tf.reduce_sum()` instead of `np.sum()`, `tf.sort()` instead of the built-in `sorted()` function, and so on (unless you really want the code to run only during tracing). This has a few additional implications:\n",
    "    * If you define a TF Function f(x) that just returns np.random.rand(), a random number will only be generated when the function is traced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the same random number, but f(tf.constant([2., 3.])) will return a different one. If you replace np.random.rand() with tf.random.uniform([]), then a new random number will be generated upon every call, since the operation will be part of the graph.\n",
    "    * If your non-TensorFlow code has side effects (such as logging something or updating a Python counter), then you should not expect those side effects to occur every time you call the TF Function, as they will only occur when the function is traced.\n",
    "    * You can wrap arbitrary Python code in a `tf.py_function()` operation, but doing so will hinder performance, as TensorFlow will not be able to do any graph optimization on this code. It will also reduce portability, as the graph will only run on platforms where Python is available (and where the right libraries are installed).\n",
    "* You can call other Python functions or TF Functions, but they should follow the same rules, as TensorFlow will capture their operations in the computation graph. Note that these other functions do not need to be decorated with `@tf.function`.\n",
    "* If the function creates a TensorFlow variable (or any other stateful TensorFlow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else you will get an exception. It is usually preferable to create variables outside of the TF Function (e.g., in the `build()` method of a custom layer). If you want to assign a new value to the variable, make sure you call its assign() method, instead of using the = operator.\n",
    "* The source code of your Python function should be available to TensorFlow. If the source code is unavailable (for example, if you define your function in the Python shell, which does not give access to the source code, or if you deploy only the compiled *.pyc Python files to production), then the graph generation process will fail or have limited functionality.\n",
    "* TensorFlow will only capture for loops that iterate over a tensor or a dataset. So make sure you use for i in `tf.range(x)` rather than `for i in range(x)`, or else the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the for loop is meant to build the graph, for example to create each layer in a neural network.)\n",
    "* As always, for performance reasons, you should prefer a vectorized implementation whenever you can, rather than using loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:studio]",
   "language": "python",
   "name": "conda-env-studio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
